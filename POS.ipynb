{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9416962-2f82-4832-80dd-cca91f6b3e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hazm in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (0.9.3)\n",
      "Requirement already satisfied: fasttext-wheel<0.10.0,>=0.9.2 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from hazm) (0.9.2)\n",
      "Requirement already satisfied: gensim<5.0.0,>=4.3.1 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from hazm) (4.3.3)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from hazm) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.3 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from hazm) (1.26.4)\n",
      "Requirement already satisfied: python-crfsuite<0.10.0,>=0.9.9 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from hazm) (0.9.11)\n",
      "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from hazm) (1.7.2)\n",
      "Requirement already satisfied: pybind11>=2.2 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (3.0.1)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (80.9.0)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from gensim<5.0.0,>=4.3.1->hazm) (7.3.1)\n",
      "Requirement already satisfied: click in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.3.0)\n",
      "Requirement already satisfied: joblib in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from nltk<4.0.0,>=3.8.1->hazm) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.67.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.6.0)\n",
      "Requirement already satisfied: wrapt in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.1->hazm) (1.17.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b1134b8-1e21-4863-9a21-7f0f552d712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19ef43e8-16c4-4315-8a74-c68c90b297f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ما', 'PRO'), ('بسیار', 'ADV'), ('کتاب', 'N'), ('می\\u200cخوانیم', 'V')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = POSTagger(model='pos_tagger.model')\n",
    "tagger.tag(word_tokenize('ما بسیار کتاب می‌خوانیم'))\n",
    "[('ما', 'PRO'), ('بسیار', 'ADV'), ('کتاب', 'N'), ('می‌خوانیم', 'V')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b7f0d2-8940-4e9d-ae35-75fbf4d782cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved -> data/pairs_pos_sample.csv\n",
      "                                             persian  \\\n",
      "0  رئیس‌جمهور تاکید کرد: کار این دولت فقط با نتیج...   \n",
      "1                          من دیگر تو را دوست ندارم.   \n",
      "2                   ما یک خانه کامل در بلوک b داریم.   \n",
      "3  عکس‌های خود را به pr@cuibul. com بفرستید یا در...   \n",
      "4                 نه. این فقط شما را عالی‌تر می‌کند.   \n",
      "5                      1396 / 12 / 9 12: 05: 56 ق. ظ   \n",
      "6                                      (39 کیلوبایت)   \n",
      "7               Herscher (Illinois) 815426 **** تلفن   \n",
      "8  همه‌ی چیزایی که «چندلر» از روی. احساس گناه گرف...   \n",
      "9                  11. موتور تون جایزه بزرگ (مسابقه)   \n",
      "\n",
      "                                         persian_pos  \n",
      "0  رئیس‌جمهور/NOUN تاکید/NOUN کرد/VERB :/PUNCT کا...  \n",
      "1  من/PRON دیگر/ADV تو/PRON را/ADP دوست/NOUN ندار...  \n",
      "2  ما/PRON یک/NUM خانه/NOUN,EZ کامل/ADJ در/ADP بل...  \n",
      "3  عکس‌های/NOUN,EZ خود/PRON را/ADP به/ADP pr@cuib...  \n",
      "4  نه/ADV ./PUNCT این/PRON فقط/ADV شما/PRON را/AD...  \n",
      "5  ۱۳۹۶/NUM //PUNCT ۱۲/NUM //PUNCT ۹/NUM ۱۲:/NUM,...  \n",
      "6               (/PUNCT ۳۹/NUM کیلوبایت/NOUN )/PUNCT  \n",
      "7  Herscher/NOUN (/PUNCT Illinois/NOUN )/PUNCT ۸۱...  \n",
      "8  همه‌ی/DET,EZ چیزایی/NOUN که/SCONJ «/PUNCT چندل...  \n",
      "9  ۱۱./NOUN موتور/NOUN,EZ تون/NOUN جایزه/NOUN,EZ ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from hazm import Normalizer, word_tokenize, POSTagger\n",
    "\n",
    "# ==== config (edit as needed) ====\n",
    "IN_CSV   = \"data/train.csv\"            # must contain a 'persian' column (and optionally 'english')\n",
    "OUT_CSV  = \"data/train_pos.csv\"\n",
    "MODEL    = \"pos_tagger.model\"         # e.g. \"resources/postagger.model\"\n",
    "DELIM    = \",\"                        # change if your CSV uses another delimiter\n",
    "NROWS    = None                       # set to an int (e.g., 10) to sample only first N rows\n",
    "# =================================\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv(IN_CSV, delimiter=DELIM, nrows=NROWS)\n",
    "\n",
    "# hazm components\n",
    "tagger = POSTagger(model=MODEL)\n",
    "norm = Normalizer(persian_numbers=True)\n",
    "\n",
    "def pos_tags_only(text: str) -> list[str]:\n",
    "    \"\"\"Return POS tag sequence (no tokens), one tag per token.\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    # normalize + tokenize with the SAME toolchain you'll use elsewhere\n",
    "    toks = word_tokenize(norm.normalize(text))\n",
    "    tagged = tagger.tag(toks)              # [(tok, POS), ...]\n",
    "    return [pos for _, pos in tagged]      # keep only POS\n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    return word_tokenize(norm.normalize(text))\n",
    "\n",
    "# build columns\n",
    "df[\"persian_tok\"] = df[\"persian\"].apply(tokenize)\n",
    "df[\"persian_pos\"] = df[\"persian\"].apply(lambda s: \" \".join(pos_tags_only(s)))\n",
    "\n",
    "# (optional) quick alignment check\n",
    "def count_ws(s): return len(str(s).split())\n",
    "align_ok = []\n",
    "bad_rows = []\n",
    "for i, (sent, tags_str) in enumerate(zip(df[\"persian\"], df[\"persian_pos\"])):\n",
    "    n_tok = len(tokenize(sent))\n",
    "    n_tag = count_ws(tags_str)\n",
    "    ok = (n_tok == n_tag)\n",
    "    align_ok.append(ok)\n",
    "    if not ok and len(bad_rows) < 10:\n",
    "        bad_rows.append((i, n_tok, n_tag, sent, tags_str))\n",
    "\n",
    "df[\"pos_align_ok\"] = align_ok\n",
    "\n",
    "# save\n",
    "cols_to_save = [c for c in [\"persian\", \"english\", \"persian_pos\"] if c in df.columns]\n",
    "df[cols_to_save].to_csv(OUT_CSV, index=False)\n",
    "print(f\"Saved -> {OUT_CSV}\")\n",
    "print(f\"Alignment OK: {sum(align_ok)}/{len(align_ok)} \"\n",
    "      f\"({100*sum(align_ok)/max(1,len(align_ok)):.1f}%)\")\n",
    "\n",
    "if bad_rows:\n",
    "    print(\"\\nFirst few misaligned rows (index, n_tok, n_tag):\")\n",
    "    for i, n_tok, n_tag, sent, tags in bad_rows:\n",
    "        print(f\"- row {i}: tokens={n_tok}, tags={n_tag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0988c364-1d92-499d-a7ed-5ff54b0d3cba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
