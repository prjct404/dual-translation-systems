{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13206453,"sourceType":"datasetVersion","datasetId":8370111}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install -U transformers datasets accelerate evaluate sentencepiece sacrebleu\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-29T21:05:36.698204Z","iopub.execute_input":"2025-09-29T21:05:36.698874Z","iopub.status.idle":"2025-09-29T21:07:27.799312Z","shell.execute_reply.started":"2025-09-29T21:05:36.698838Z","shell.execute_reply":"2025-09-29T21:07:27.798571Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.6/503.6 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.9/374.9 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#setup and imports","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T21:07:27.800750Z","iopub.execute_input":"2025-09-29T21:07:27.800982Z","iopub.status.idle":"2025-09-29T21:07:27.804967Z","shell.execute_reply.started":"2025-09-29T21:07:27.800960Z","shell.execute_reply":"2025-09-29T21:07:27.804400Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# !pip -q install sentencepiece sacrebleu datasets kagglehub\n\nimport os, math, random, time\nimport numpy as np\nimport torch, torch.nn as nn\nfrom typing import List, Tuple\nfrom datasets import Dataset, DatasetDict\nimport sentencepiece as spm\nimport sacrebleu\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); \nif DEVICE==\"cuda\": torch.cuda.manual_seed_all(SEED)\n\nprint(\"Device:\", DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T21:07:27.805627Z","iopub.execute_input":"2025-09-29T21:07:27.805837Z","iopub.status.idle":"2025-09-29T21:07:34.821068Z","shell.execute_reply.started":"2025-09-29T21:07:27.805821Z","shell.execute_reply":"2025-09-29T21:07:34.820362Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Expecting train.csv, valid.csv (and optionally test.csv)\nimport kagglehub\nfrom kagglehub import KaggleDatasetAdapter\n\n\n# Set the path to the file you'd like to load\nfile_path = \"test.csv\"\n\n# Load the latest version\ndf = kagglehub.load_dataset(\n  KaggleDatasetAdapter.PANDAS,\n  \"arefehrajabian/persian-pos-ner\",\n  file_path,\n\n)\n\nprint(\"First 5 records:\", df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T21:07:34.821943Z","iopub.execute_input":"2025-09-29T21:07:34.822392Z","iopub.status.idle":"2025-09-29T21:07:35.115356Z","shell.execute_reply.started":"2025-09-29T21:07:34.822368Z","shell.execute_reply":"2025-09-29T21:07:35.114628Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/1068884255.py:10: DeprecationWarning: load_dataset is deprecated and will be removed in a future version.\n  df = kagglehub.load_dataset(\n","output_type":"stream"},{"name":"stdout","text":"First 5 records:                                              persian  \\\n0                   و به زیر لیوانی کاغذی من نگاه کن   \n1                           تو برو یه همچین کاری بکن   \n2                                              پذیرش   \n3  مدیامکس اخبار قره باغ کوهستانی وزیر خارجه ارمن...   \n4  صد خانه در آن واحد متعارف و ترسناک در زیر آسما...   \n\n                                             english  \\\n0         and look down at my embossed paper coaster   \n1                  you go and do something like this   \n2                                            adopcje   \n3  mediamax news nagorno karabakh armenian fm bak...   \n4  a hundred houses at once conventional and grot...   \n\n                                         persian_pos  \\\n0       CCONJ ADP NOUN,EZ NOUN ADJ,EZ PRON NOUN VERB   \n1                       PRON VERB NUM NOUN NOUN VERB   \n2                                               NOUN   \n3  NOUN NOUN,EZ NOUN,EZ ADJ,EZ NOUN,EZ ADJ,EZ NOU...   \n4  NUM NOUN ADP NOUN,EZ NOUN,EZ ADJ CCONJ ADJ ADP...   \n\n                                          fa_ner_seq  \n0                                    0 0 0 0 0 0 0 0  \n1                                        0 0 0 0 0 0  \n2                                                  0  \n3  ORG 0 LOCATION LOCATION 0 0 ORG ORG 0 LOCATION...  \n4          0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  \n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# !pip -q install kagglehub datasets\n\nimport pandas as pd\nimport kagglehub\nfrom kagglehub import KaggleDatasetAdapter\nfrom datasets import Dataset, DatasetDict\n\nSLUG = \"arefehrajabian/persian-pos-ner\"   # your dataset\n\n# Adjust these filenames if your split names differ\nTRAIN_FILE = \"train.csv\"\nVALID_FILE = \"validation.csv\"      # change to \"validation.csv\" or \"dev.csv\" if needed\nTEST_FILE  = \"test.csv\"       # optional\n\n# Map the dataset's column names -> our standard 'src'/'tgt'\nSRC_COL_IN = \"english\"        # <- source (EN)\nTGT_COL_IN = \"persian\"        # <- target (FA)\n\ndef safe_load_csv(slug, file_path):\n    try:\n        return kagglehub.load_dataset(\n            KaggleDatasetAdapter.PANDAS,\n            slug,\n            file_path,\n        )\n    except Exception as e:\n        print(f\"Skipping '{file_path}' — not found or not a CSV. ({e})\")\n        return None\n\ndfs = {\n    \"train\":      safe_load_csv(SLUG, TRAIN_FILE),\n    \"validation\": safe_load_csv(SLUG, VALID_FILE),\n    \"test\":       safe_load_csv(SLUG, TEST_FILE),\n}\ndfs = {k: v for k, v in dfs.items() if v is not None}\n\nfor split, df in dfs.items():\n    print(f\"{split}: shape={df.shape}, cols={list(df.columns)}\")\n\ndef standardize(df, src_in=SRC_COL_IN, tgt_in=TGT_COL_IN):\n    if src_in not in df.columns or tgt_in not in df.columns:\n        raise ValueError(f\"Expected columns '{src_in}' and '{tgt_in}', found {list(df.columns)}\")\n    out = df[[src_in, tgt_in]].rename(columns={src_in: \"src\", tgt_in: \"tgt\"})\n    out = out.dropna(subset=[\"src\", \"tgt\"]).drop_duplicates().reset_index(drop=True)\n    return out\n\ndfs_std = {split: standardize(df) for split, df in dfs.items()}\n\n# Convert to HF datasets\nraw = DatasetDict({split: Dataset.from_pandas(df.reset_index(drop=True)) for split, df in dfs_std.items()})\n\n# Keep the rest of your pipeline expecting these names:\nSRC_COL, TGT_COL = \"src\", \"tgt\"\nTEST_REF_COL = \"ref\"  # only relevant if your test file has references\n\nprint(\"\\nFinal splits:\")\nfor split in raw.keys():\n    print(split, \"→\", raw[split].num_rows, raw[split].column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T21:07:35.117555Z","iopub.execute_input":"2025-09-29T21:07:35.117832Z","iopub.status.idle":"2025-09-29T21:07:39.365241Z","shell.execute_reply.started":"2025-09-29T21:07:35.117815Z","shell.execute_reply":"2025-09-29T21:07:39.364438Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/2724337234.py:21: DeprecationWarning: load_dataset is deprecated and will be removed in a future version.\n  return kagglehub.load_dataset(\n/tmp/ipykernel_36/2724337234.py:21: DeprecationWarning: load_dataset is deprecated and will be removed in a future version.\n  return kagglehub.load_dataset(\n/tmp/ipykernel_36/2724337234.py:21: DeprecationWarning: load_dataset is deprecated and will be removed in a future version.\n  return kagglehub.load_dataset(\n","output_type":"stream"},{"name":"stdout","text":"train: shape=(136396, 4), cols=['persian', 'english', 'persian_pos', 'fa_ner_seq']\nvalidation: shape=(5990, 4), cols=['persian', 'english', 'persian_pos', 'fa_ner_seq']\ntest: shape=(5991, 4), cols=['persian', 'english', 'persian_pos', 'fa_ner_seq']\n\nFinal splits:\ntrain → 136158 ['src', 'tgt']\nvalidation → 5989 ['src', 'tgt']\ntest → 5991 ['src', 'tgt']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"**1) Train a shared SentencePiece tokenizer (BPE/Unigram)**","metadata":{}},{"cell_type":"code","source":"# Collect text for tokenizer training (subset if dataset is huge)\ndef iter_text(ds, col):\n    for x in ds[col]:\n        if x is None: \n            continue\n        s = str(x).strip()\n        if s: \n            yield s\n\n# Feed both EN+FA so we get a shared vocab\ntrain_src = list(iter_text(raw[\"train\"], \"src\"))\ntrain_tgt = list(iter_text(raw[\"train\"], \"tgt\"))\ntok_train_path = \"/kaggle/working/spm_corpus.txt\"\nwith open(tok_train_path, \"w\", encoding=\"utf-8\") as f:\n    for s in train_src + train_tgt:\n        f.write(s.replace(\"\\n\",\" \") + \"\\n\")\n\nVOCAB_SIZE = 16000\nSPM_PREFIX = \"/kaggle/working/spm_en_fa\"\nspm.SentencePieceTrainer.Train(\n    input=tok_train_path, \n    model_prefix=SPM_PREFIX, \n    vocab_size=VOCAB_SIZE,\n    model_type=\"unigram\",             # or \"bpe\"\n    character_coverage=1.0,\n    pad_id=0, unk_id=1, bos_id=2, eos_id=3,  # reserve ids\n    input_sentence_size=2000000,      # subsample if very large\n    shuffle_input_sentence=True\n)\n\nsp = spm.SentencePieceProcessor()\nsp.Load(f\"{SPM_PREFIX}.model\")\n\nPAD, UNK, BOS, EOS = 0, 1, 2, 3\nprint(\"SentencePiece loaded. Example pieces:\", sp.EncodeAsPieces(\"Hello Tehran!\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T21:07:39.366083Z","iopub.execute_input":"2025-09-29T21:07:39.366362Z","iopub.status.idle":"2025-09-29T21:08:01.399262Z","shell.execute_reply.started":"2025-09-29T21:07:39.366335Z","shell.execute_reply":"2025-09-29T21:08:01.398402Z"}},"outputs":[{"name":"stdout","text":"SentencePiece loaded. Example pieces: ['▁', 'H', 'el', 'lo', '▁', 'T', 'e', 'hran', '!']\n","output_type":"stream"},{"name":"stderr","text":"sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \ntrainer_spec {\n  input: /kaggle/working/spm_corpus.txt\n  input_format: \n  model_prefix: /kaggle/working/spm_en_fa\n  model_type: UNIGRAM\n  vocab_size: 16000\n  self_test_sample_size: 0\n  character_coverage: 1\n  input_sentence_size: 2000000\n  shuffle_input_sentence: 1\n  seed_sentencepiece_size: 1000000\n  shrinking_factor: 0.75\n  max_sentence_length: 4192\n  num_threads: 16\n  num_sub_iterations: 2\n  max_sentencepiece_length: 16\n  split_by_unicode_script: 1\n  split_by_number: 1\n  split_by_whitespace: 1\n  split_digits: 0\n  pretokenization_delimiter: \n  treat_whitespace_as_suffix: 0\n  allow_whitespace_only_pieces: 0\n  required_chars: \n  byte_fallback: 0\n  vocabulary_output_piece_score: 1\n  train_extremely_large_corpus: 0\n  seed_sentencepieces_file: \n  hard_vocab_limit: 1\n  use_all_vocab: 0\n  unk_id: 1\n  bos_id: 2\n  eos_id: 3\n  pad_id: 0\n  unk_piece: <unk>\n  bos_piece: <s>\n  eos_piece: </s>\n  pad_piece: <pad>\n  unk_surface:  ⁇ \n  enable_differential_privacy: 0\n  differential_privacy_noise_level: 0\n  differential_privacy_clipping_threshold: 0\n}\nnormalizer_spec {\n  name: nmt_nfkc\n  add_dummy_prefix: 1\n  remove_extra_whitespaces: 1\n  escape_whitespaces: 1\n  normalization_rule_tsv: \n}\ndenormalizer_spec {}\ntrainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\ntrainer_interface.cc(186) LOG(INFO) Loading corpus: /kaggle/working/spm_corpus.txt\ntrainer_interface.cc(411) LOG(INFO) Loaded all 272316 sentences\ntrainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\ntrainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\ntrainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\ntrainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\ntrainer_interface.cc(432) LOG(INFO) Normalizing sentences...\ntrainer_interface.cc(541) LOG(INFO) all chars count=19782616\ntrainer_interface.cc(562) LOG(INFO) Alphabet size=73\ntrainer_interface.cc(563) LOG(INFO) Final character coverage=1\ntrainer_interface.cc(594) LOG(INFO) Done! preprocessed 272316 sentences.\nunigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\nunigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=9968589\nunigram_model_trainer.cc(312) LOG(INFO) Initialized 217461 seed sentencepieces\ntrainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 272316\ntrainer_interface.cc(611) LOG(INFO) Done! 107523\nunigram_model_trainer.cc(602) LOG(INFO) Using 107523 sentences for EM training\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=87725 obj=10.193 num_tokens=184191 num_tokens/piece=2.09964\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=70670 obj=8.48617 num_tokens=184261 num_tokens/piece=2.60734\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=52994 obj=8.45226 num_tokens=196821 num_tokens/piece=3.71402\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=52957 obj=8.43984 num_tokens=196883 num_tokens/piece=3.71779\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=39716 obj=8.49395 num_tokens=215755 num_tokens/piece=5.43245\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=39713 obj=8.47979 num_tokens=215753 num_tokens/piece=5.43281\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=29784 obj=8.56433 num_tokens=236817 num_tokens/piece=7.95115\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=29784 obj=8.54582 num_tokens=236819 num_tokens/piece=7.95122\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=22338 obj=8.66709 num_tokens=258914 num_tokens/piece=11.5907\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=22338 obj=8.642 num_tokens=258915 num_tokens/piece=11.5908\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=17600 obj=8.77507 num_tokens=276357 num_tokens/piece=15.7021\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=17600 obj=8.74837 num_tokens=276377 num_tokens/piece=15.7032\ntrainer_interface.cc(689) LOG(INFO) Saving model: /kaggle/working/spm_en_fa.model\ntrainer_interface.cc(701) LOG(INFO) Saving vocabs: /kaggle/working/spm_en_fa.vocab\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"**2) Encode datasets → PyTorch-friendly Datasets + DataLoaders**","metadata":{}},{"cell_type":"code","source":"from typing import List, Tuple\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch\n\n\nMAX_SRC_LEN = 160\nMAX_TGT_LEN = 160\nBATCH_SIZE  = 64   # adjust if OOM\n\ndef encode_example(src: str, tgt: str) -> Tuple[List[int], List[int]]:\n    src_ids = sp.EncodeAsIds(str(src))[:MAX_SRC_LEN-1] + [EOS]\n    tgt_ids = [BOS] + sp.EncodeAsIds(str(tgt))[:MAX_TGT_LEN-2] + [EOS]\n    return src_ids, tgt_ids\n\nclass MTEncodedDataset(Dataset):\n    def __init__(self, hf_split):\n        self.src = hf_split[\"src\"]\n        self.tgt = hf_split[\"tgt\"]\n    def __len__(self): return len(self.src)\n    def __getitem__(self, i):\n        return encode_example(self.src[i], self.tgt[i])\n\ndef collate(batch):\n    # batch: list of (src_ids, tgt_ids)\n    src_seqs, tgt_seqs = zip(*batch)\n    src_lens = torch.tensor([len(s) for s in src_seqs])\n    tgt_lens = torch.tensor([len(t) for t in tgt_seqs])\n\n    def pad_to_max(seqs, pad=PAD):\n        maxlen = max(len(s) for s in seqs)\n        out = torch.full((maxlen, len(seqs)), pad, dtype=torch.long)\n        for i, s in enumerate(seqs):\n            out[:len(s), i] = torch.tensor(s, dtype=torch.long)\n        return out\n\n    src_pad = pad_to_max(src_seqs, PAD)  # (T,B)\n    tgt_pad = pad_to_max(tgt_seqs, PAD)  # (T,B)\n    return src_pad, src_lens, tgt_pad, tgt_lens\n\nds_train = MTEncodedDataset(raw[\"train\"])\nds_valid = MTEncodedDataset(raw[\"validation\"])\ndl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate, drop_last=False)\ndl_valid = DataLoader(ds_valid, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate, drop_last=False)\nlen(ds_train), len(ds_valid)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T21:08:01.400028Z","iopub.execute_input":"2025-09-29T21:08:01.400223Z","iopub.status.idle":"2025-09-29T21:08:01.417551Z","shell.execute_reply.started":"2025-09-29T21:08:01.400204Z","shell.execute_reply":"2025-09-29T21:08:01.416970Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(136158, 5989)"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"**Build the LSTM seq2seq with additive attention**","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hid_dim, layers=1, dropout=0.2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=layers, bidirectional=True,\n                           dropout=0 if layers==1 else dropout)\n        self.dropout = nn.Dropout(dropout)\n        self.hid_dim = hid_dim\n        self.layers = layers\n\n    def forward(self, src, src_lens):\n        # src: (T,B)\n        emb = self.dropout(self.emb(src))  # (T,B,E)\n        packed = nn.utils.rnn.pack_padded_sequence(emb, src_lens.cpu(), enforce_sorted=False)\n        out, (h, c) = self.rnn(packed)\n        out, _ = nn.utils.rnn.pad_packed_sequence(out)  # (T,B,2H)\n        # reshape h,c: (2*L,B,H) -> (L,B,2H)\n        h = torch.cat([h[0::2], h[1::2]], dim=2)\n        c = torch.cat([c[0::2], c[1::2]], dim=2)\n        return out, (h, c)  # out for attention\n\n# ---------- 1) Attention patched for AMP (no fp16 overflow) ----------\nimport torch, torch.nn as nn\n\nclass AdditiveAttention(nn.Module):\n    def __init__(self, enc_dim, dec_dim, attn_dim=256):\n        super().__init__()\n        self.W = nn.Linear(enc_dim + dec_dim, attn_dim)\n        self.v = nn.Linear(attn_dim, 1, bias=False)\n\n    def forward(self, dec_hidden, enc_outs, src_mask):\n        # dec_hidden: (B,Hd), enc_outs: (T,B,He), src_mask: (B,T) bool\n        T, B, He = enc_outs.size()\n        orig_dtype = enc_outs.dtype\n\n        # build energy in original dtype\n        dec = dec_hidden.unsqueeze(0).expand(T, B, -1)                # (T,B,Hd)\n        energy = torch.tanh(self.W(torch.cat([dec, enc_outs], dim=2)))# (T,B,A)\n\n        # scores/softmax in float32 to avoid fp16 overflow\n        scores = self.v(energy).squeeze(2).transpose(0,1).float()     # (B,T), fp32\n\n        # mask safely (fp32)\n        fill_val = -1e4 if scores.dtype == torch.float16 else -1e9\n        scores = scores.masked_fill(~src_mask, fill_val)\n\n        attn = torch.softmax(scores, dim=1)                            # (B,T), fp32\n        # context in fp32, then cast back to original dtype\n        context = torch.bmm(attn.unsqueeze(1), enc_outs.transpose(0,1).float()).squeeze(1)  # (B,He) fp32\n        context = context.to(orig_dtype)\n        return context, attn\n\n\n\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, enc_dim, dec_hid, layers=1, dropout=0.2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n        self.rnn = nn.LSTM(emb_dim + enc_dim, dec_hid, num_layers=layers,\n                           dropout=0 if layers==1 else dropout)\n        self.attn = AdditiveAttention(enc_dim, dec_hid)\n        self.fc = nn.Linear(dec_hid + enc_dim + emb_dim, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, y_prev, hidden, cell, enc_outs, src_mask):\n        emb = self.dropout(self.emb(y_prev)).unsqueeze(0)   # (1,B,E)\n        dec_hidden = hidden[-1]                             # (B,Hd)\n        context, _ = self.attn(dec_hidden, enc_outs, src_mask)  # (B,He)\n        rnn_in = torch.cat([emb, context.unsqueeze(0)], dim=2)  # (1,B,E+He)\n        out, (h,c) = self.rnn(rnn_in, (hidden, cell))           # out: (1,B,Hd)\n        out = out.squeeze(0)                                    # (B,Hd)\n        logits = self.fc(torch.cat([out, context, emb.squeeze(0)], dim=1))  # (B,V)\n        return logits, (h,c)\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, vocab_size, emb=256, enc_h=384, dec_h=512, enc_layers=1, dec_layers=1, dropout=0.2):\n        super().__init__()\n        self.enc = Encoder(vocab_size, emb, enc_h, enc_layers, dropout)\n        self.dec = Decoder(vocab_size, emb, enc_h*2, dec_h, dec_layers, dropout)\n        # (optional) tie decoder output to embeddings for smaller models\n        # self.dec.fc.weight = self.dec.emb.weight\n\n    def make_src_mask(self, src):\n        return (src.transpose(0,1) != PAD)   # (B,T)\n\n    def forward(self, src, src_lens, tgt, teacher_forcing=0.8):\n        enc_outs, (h,c) = self.enc(src, src_lens)            # enc_outs: (T,B,2Henc)\n        src_mask = self.make_src_mask(src)                   # (B,T)\n        B = src.size(1)\n        y = tgt[0]  # BOS (assuming first token is BOS)\n        logits_seq = []\n        # project encoder state to decoder dims if needed (here we keep simple: zeros)\n        hidden = torch.zeros(self.dec.rnn.num_layers, B, self.dec.rnn.hidden_size, device=src.device)\n        cell   = torch.zeros(self.dec.rnn.num_layers, B, self.dec.rnn.hidden_size, device=src.device)\n        for t in range(1, tgt.size(0)):\n            logits, (hidden, cell) = self.dec(y, hidden, cell, enc_outs, src_mask)\n            logits_seq.append(logits.unsqueeze(0))\n            y = tgt[t] if (random.random()<teacher_forcing) else logits.argmax(dim=1)\n        return torch.cat(logits_seq, dim=0)   # (T-1,B,V)\n\n    @torch.no_grad()\n    def generate(self, src, src_lens, max_len=160, beam_size=1):\n        self.eval()\n        enc_outs, (h,c) = self.enc(src, src_lens)\n        src_mask = self.make_src_mask(src)\n        B = src.size(1)\n\n        # Greedy only (beam=1) for simplicity here\n        y = torch.full((B,), BOS, dtype=torch.long, device=src.device)\n        hidden = torch.zeros(self.dec.rnn.num_layers, B, self.dec.rnn.hidden_size, device=src.device)\n        cell   = torch.zeros(self.dec.rnn.num_layers, B, self.dec.rnn.hidden_size, device=src.device)\n        outs = []\n        for _ in range(max_len):\n            logits, (hidden, cell) = self.dec(y, hidden, cell, enc_outs, src_mask)\n            y = logits.argmax(dim=1)\n            outs.append(y)\n            if (y==EOS).all(): break\n        return torch.stack(outs, 0)  # (T,B)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T21:08:01.418480Z","iopub.execute_input":"2025-09-29T21:08:01.418800Z","iopub.status.idle":"2025-09-29T21:08:01.446111Z","shell.execute_reply.started":"2025-09-29T21:08:01.418775Z","shell.execute_reply":"2025-09-29T21:08:01.445537Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n# you already have `sp` and `Seq2Seq` defined/imported\n\n# --- hyperparameters (set to whatever you want)\nEMB_DIM = 512\nENC_H   = 512\nDEC_H   = 512\nLR      = 3e-4\nLABEL_SMOOTH = 0.1\n\n# --- special token ids\npad_id = getattr(sp, \"PieceToId\")(\"<pad>\") if hasattr(sp, \"PieceToId\") else -1\nPAD = pad_id if pad_id != -1 else 0  # fallback if your model has no <pad> piece\n\n# --- device\n# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T21:08:01.446833Z","iopub.execute_input":"2025-09-29T21:08:01.447080Z","iopub.status.idle":"2025-09-29T21:08:01.470623Z","shell.execute_reply.started":"2025-09-29T21:08:01.447063Z","shell.execute_reply":"2025-09-29T21:08:01.470018Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"**train the model**","metadata":{}},{"cell_type":"code","source":"# VOCAB_SIZE = sp.GetPieceSize()\n# model = Seq2Seq(VOCAB_SIZE, emb=EMB_DIM, enc_h=ENC_H, dec_h=DEC_H, enc_layers=1, dec_layers=1, dropout=0.2).to(DEVICE)\n# optim = torch.optim.AdamW(model.parameters(), lr=LR)\n# try:\n#     crit = nn.CrossEntropyLoss(ignore_index=PAD, label_smoothing=LABEL_SMOOTH)\n# except TypeError:\n#     crit = nn.CrossEntropyLoss(ignore_index=PAD)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T21:08:01.471321Z","iopub.execute_input":"2025-09-29T21:08:01.471559Z","iopub.status.idle":"2025-09-29T21:08:01.494922Z","shell.execute_reply.started":"2025-09-29T21:08:01.471542Z","shell.execute_reply":"2025-09-29T21:08:01.494247Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"**first try(interrupt in second epoch)**","metadata":{}},{"cell_type":"code","source":"# ===== Fix & full training block (safe, self-contained) =====\nimport math, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List, Tuple\n\n# --- sanity checks (raw + sp must exist) ---\nassert 'train' in raw and 'validation' in raw, \"Expected raw['train'] and raw['validation']\"\nfor split in ['train','validation']:\n    assert 'src' in raw[split].column_names and 'tgt' in raw[split].column_names, \"raw must have 'src' and 'tgt'\"\nassert 'EncodeAsIds' in dir(sp), \"SentencePiece processor `sp` not loaded\"\n\n# --- special tokens (match your SPM training) ---\nPAD, UNK, BOS, EOS = 0, 1, 2, 3\n\n# --- hyperparams (edit as you like) ---\nMAX_SRC_LEN = 160\nMAX_TGT_LEN = 160\nBATCH_SIZE  = 64\nEMB_DIM = 256\nENC_H   = 384\nDEC_H   = 512\nEPOCHS  = 5\nLR      = 3e-4\nCLIP    = 1.0\nLABEL_SMOOTH = 0.1\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# --- dataset & collate that ALWAYS returns a 4-tuple ---\ndef _encode_example(src: str, tgt: str) -> Tuple[list, list]:\n    src_ids = sp.EncodeAsIds(str(src))[:MAX_SRC_LEN-1] + [EOS]\n    tgt_ids = [BOS] + sp.EncodeAsIds(str(tgt))[:MAX_TGT_LEN-2] + [EOS]\n    return src_ids, tgt_ids\n\nclass MTEncodedDataset(Dataset):\n    def __init__(self, hf_split):\n        self.src = hf_split[\"src\"]\n        self.tgt = hf_split[\"tgt\"]\n    def __len__(self): return len(self.src)\n    def __getitem__(self, i):\n        return _encode_example(self.src[i], self.tgt[i])\n\ndef collate(batch):\n    # batch: list of (src_ids, tgt_ids)\n    src_seqs, tgt_seqs = zip(*batch)\n    src_lens = torch.tensor([len(s) for s in src_seqs], dtype=torch.long)\n    tgt_lens = torch.tensor([len(t) for t in tgt_seqs], dtype=torch.long)\n    def pad_to_max(seqs, pad=PAD):\n        T = max(len(s) for s in seqs)\n        B = len(seqs)\n        out = torch.full((T, B), pad, dtype=torch.long)\n        for i, s in enumerate(seqs):\n            out[:len(s), i] = torch.tensor(s, dtype=torch.long)\n        return out\n    src_pad = pad_to_max(src_seqs, PAD)  # (T,B)\n    tgt_pad = pad_to_max(tgt_seqs, PAD)  # (T,B)\n    return src_pad, src_lens, tgt_pad, tgt_lens  # <<< 4-tuple\n\n# --- build loaders fresh (guarantees tuple batches) ---\nds_train = MTEncodedDataset(raw[\"train\"])\nds_valid = MTEncodedDataset(raw[\"validation\"])\ndl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate, drop_last=False)\ndl_valid = DataLoader(ds_valid, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate, drop_last=False)\n\n# --- quick diagnostic (optional) ---\nfirst = next(iter(dl_train))\nassert isinstance(first, (list, tuple)) and len(first) == 4, \"Batch must be a 4-tuple (src, src_lens, tgt, tgt_lens)\"\n\n# --- model, optim, loss ---\nVOCAB_SIZE = sp.GetPieceSize()\nmodel = Seq2Seq(VOCAB_SIZE, emb=EMB_DIM, enc_h=ENC_H, dec_h=DEC_H, enc_layers=1, dec_layers=1, dropout=0.2).to(DEVICE)\noptim = torch.optim.AdamW(model.parameters(), lr=LR)\ntry:\n    crit = nn.CrossEntropyLoss(ignore_index=PAD, label_smoothing=LABEL_SMOOTH)\nexcept TypeError:\n    crit = nn.CrossEntropyLoss(ignore_index=PAD)\n\n# --- robust unpack + epoch runner ---\ndef _unpack_batch(batch):\n    # batch is guaranteed a 4-tuple from our collate; keep flexible anyway\n    if isinstance(batch, (list, tuple)) and len(batch) >= 3:\n        src, src_lens, tgt = batch[0], batch[1], batch[2]\n        return src, src_lens, tgt\n    elif isinstance(batch, dict):\n        src = batch.get(\"src\"); src_lens = batch.get(\"src_lens\")\n        tgt = batch.get(\"tgt\") or batch.get(\"labels\")\n        if src is None or src_lens is None or tgt is None:\n            raise ValueError(f\"Unexpected batch dict keys: {list(batch.keys())}\")\n        return src, src_lens, tgt\n    else:\n        raise TypeError(f\"Unsupported batch type: {type(batch)}\")\n\ndef run_epoch(dataloader, train=True):\n    model.train() if train else model.eval()\n    total_loss, total_tokens = 0.0, 0\n    for batch in dataloader:\n        src, src_lens, tgt = _unpack_batch(batch)\n        src, src_lens, tgt = src.to(DEVICE), src_lens.to(DEVICE), tgt.to(DEVICE)\n\n        if train:\n            logits = model(src, src_lens, tgt, teacher_forcing=0.8)   # (T-1,B,V)\n        else:\n            with torch.no_grad():\n                logits = model(src, src_lens, tgt, teacher_forcing=1.0)\n\n        gold = tgt[1:].contiguous().view(-1)  # shift\n        loss = crit(logits.view(-1, logits.size(-1)), gold)\n\n        if train:\n            optim.zero_grad()\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n            optim.step()\n\n        ntoks = (gold != PAD).sum().item()\n        total_loss += float(loss) * ntoks\n        total_tokens += ntoks\n\n    return total_loss / max(total_tokens, 1)\n\n# --- train loop (saves best by val loss) ---\nbest_val = float(\"inf\")\nbest_path = \"/kaggle/working/seq2seq_best.pt\"\nfor epoch in range(1, EPOCHS+1):\n    tr_loss = run_epoch(dl_train, train=True)\n    val_loss = run_epoch(dl_valid, train=False)\n    print(f\"Epoch {epoch}: train_ppl={math.exp(tr_loss):.2f}  val_ppl={math.exp(val_loss):.2f}\")\n    if val_loss < best_val:\n        best_val = val_loss\n        torch.save(model.state_dict(), best_path)\n        print(\"Saved best checkpoint ->\", best_path)\n\nprint(\"Done.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T21:08:01.495733Z","iopub.execute_input":"2025-09-29T21:08:01.495948Z"}},"outputs":[{"name":"stdout","text":"Epoch 1: train_ppl=463.23  val_ppl=183.27\nSaved best checkpoint -> /kaggle/working/seq2seq_best.pt\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# continue training","metadata":{}},{"cell_type":"code","source":"# import os, time, math, torch, torch.nn as nn\n\n# best_path = \"/kaggle/working/seq2seq_best.pt\"\n# last_path = \"/kaggle/working/seq2seq_last.pt\"\n\n# # ---------- Encoder/Decoder with fp32-safe attention ----------\n# class Encoder(nn.Module):\n#     def __init__(self, vocab_size, emb_dim, hid_dim, layers=1, dropout=0.2):\n#         super().__init__()\n#         self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n#         self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=layers, bidirectional=True,\n#                            dropout=0 if layers==1 else dropout)\n#         self.dropout = nn.Dropout(dropout)\n#     def forward(self, src, src_lens):\n#         emb = self.dropout(self.emb(src))                          # (T,B,E)\n#         packed = nn.utils.rnn.pack_padded_sequence(emb, src_lens.cpu(), enforce_sorted=False)\n#         out,(h,c) = self.rnn(packed)\n#         out,_ = nn.utils.rnn.pad_packed_sequence(out)              # (T,B,2H)\n#         h = torch.cat([h[0::2], h[1::2]], dim=2)                   # (L,B,2H)\n#         c = torch.cat([c[0::2], c[1::2]], dim=2)\n#         return out,(h,c)\n\n# class AdditiveAttention(nn.Module):\n#     def __init__(self, enc_dim, dec_dim, attn_dim=256):\n#         super().__init__()\n#         self.W = nn.Linear(enc_dim + dec_dim, attn_dim)\n#         self.v = nn.Linear(attn_dim, 1, bias=False)\n#     def forward(self, dec_hidden, enc_outs, src_mask):\n#         # enc_outs: (T,B,He)  dec_hidden: (B,Hd)  src_mask: (B,T) bool\n#         T,B,He = enc_outs.size()\n#         orig_dtype = enc_outs.dtype\n#         dec = dec_hidden.unsqueeze(0).expand(T,B,-1)               # (T,B,Hd)\n#         energy = torch.tanh(self.W(torch.cat([dec, enc_outs], dim=2)))\n#         scores = self.v(energy).squeeze(2).transpose(0,1).float()  # (B,T), fp32\n#         scores = scores.masked_fill(~src_mask, -1e9)               # mask in fp32\n#         attn   = torch.softmax(scores, dim=1)                      # (B,T), fp32\n#         context = torch.bmm(attn.unsqueeze(1), enc_outs.transpose(0,1).float()).squeeze(1)  # (B,He), fp32\n#         return context.to(orig_dtype), attn\n\n# class Decoder(nn.Module):\n#     def __init__(self, vocab_size, emb_dim, enc_dim, dec_hid, layers=1, dropout=0.2):\n#         super().__init__()\n#         self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n#         self.rnn = nn.LSTM(emb_dim + enc_dim, dec_hid, num_layers=layers,\n#                            dropout=0 if layers==1 else dropout)\n#         self.attn = AdditiveAttention(enc_dim, dec_hid)\n#         self.fc = nn.Linear(dec_hid + enc_dim + emb_dim, vocab_size)\n#         self.dropout = nn.Dropout(dropout)\n#     def forward(self, y_prev, hidden, cell, enc_outs, src_mask):\n#         emb = self.dropout(self.emb(y_prev)).unsqueeze(0)          # (1,B,E)\n#         dec_hidden = hidden[-1]                                     # (B,Hd)\n#         context,_ = self.attn(dec_hidden, enc_outs, src_mask)       # (B,He)\n#         rnn_in = torch.cat([emb, context.unsqueeze(0)], dim=2)      # (1,B,E+He)\n#         out,(h,c) = self.rnn(rnn_in, (hidden, cell))                # out: (1,B,Hd)\n#         out = out.squeeze(0)                                        # (B,Hd)\n#         logits = self.fc(torch.cat([out, context, emb.squeeze(0)], dim=1))  # (B,V)\n#         return logits,(h,c)\n\n# class Seq2Seq(nn.Module):\n#     def __init__(self, vocab_size, emb=256, enc_h=384, dec_h=512, enc_layers=1, dec_layers=1, dropout=0.2):\n#         super().__init__()\n#         self.enc = Encoder(vocab_size, emb, enc_h, enc_layers, dropout)\n#         self.dec = Decoder(vocab_size, emb, enc_h*2, dec_h, dec_layers, dropout)\n#     def make_src_mask(self, src):\n#         # src: (T,B) -> mask: (B,T) bool\n#         return (src.transpose(0,1) != PAD).bool()\n#     def forward(self, src, src_lens, tgt, teacher_forcing=0.8):\n#         enc_outs,(h,c) = self.enc(src, src_lens)\n#         mask = self.make_src_mask(src)\n#         B = src.size(1)\n#         y = tgt[0]  # BOS\n#         hidden = torch.zeros(self.dec.rnn.num_layers, B, self.dec.rnn.hidden_size, device=src.device)\n#         cell   = torch.zeros(self.dec.rnn.num_layers, B, self.dec.rnn.hidden_size, device=src.device)\n#         outs=[]\n#         for t in range(1, tgt.size(0)):\n#             logits,(hidden,cell) = self.dec(y, hidden, cell, enc_outs, mask)\n#             outs.append(logits.unsqueeze(0))\n#             y = tgt[t] if (torch.rand(1, device=src.device).item() < teacher_forcing) else logits.argmax(dim=1)\n#         return torch.cat(outs, 0)\n#     @torch.no_grad()\n#     def generate(self, src, src_lens, max_len=160):\n#         self.eval()\n#         enc_outs,(h,c) = self.enc(src, src_lens)\n#         mask = self.make_src_mask(src)\n#         B = src.size(1)\n#         y = torch.full((B,), BOS, dtype=torch.long, device=src.device)\n#         hidden = torch.zeros(self.dec.rnn.num_layers, B, self.dec.rnn.hidden_size, device=src.device)\n#         cell   = torch.zeros(self.dec.rnn.num_layers, B, self.dec.rnn.hidden_size, device=src.device)\n#         outs=[]\n#         for _ in range(max_len):\n#             logits,(hidden,cell) = self.dec(y, hidden, cell, enc_outs, mask)\n#             y = logits.argmax(dim=1)\n#             outs.append(y)\n#             if (y==EOS).all(): break\n#         return torch.stack(outs,0)\n\n# # ---------- rebuild model and RESUME ----------\n# VOCAB_SIZE = sp.GetPieceSize()\n# model = Seq2Seq(VOCAB_SIZE, emb=EMB_DIM, enc_h=ENC_H, dec_h=DEC_H).to(DEVICE)\n\n# # (re)create optimizer; we'll load state if resuming from \"last\"\n# optim = torch.optim.AdamW(model.parameters(), lr=LR)\n\n# # new AMP API\n# scaler = torch.amp.GradScaler('cuda', enabled=(DEVICE=='cuda'))\n\n# start_epoch = 1\n# if os.path.exists(last_path):\n#     ckpt = torch.load(last_path, map_location=DEVICE)\n#     model.load_state_dict(ckpt[\"model\"])\n#     try: optim.load_state_dict(ckpt[\"optim\"])\n#     except: pass\n#     try: scaler.load_state_dict(ckpt[\"scaler\"])\n#     except: pass\n#     start_epoch = ckpt.get(\"epoch\", 0) + 1\n#     print(f\"[resume] Loaded LAST checkpoint at epoch {start_epoch-1}\")\n# elif os.path.exists(best_path):\n#     model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n#     print(\"[resume] Loaded BEST checkpoint\")\n# else:\n#     print(\"[resume] No checkpoint found; starting fresh\")\n\n# # ---------- safe epoch runner ----------\n# def run_epoch(dataloader, train=True):\n#     model.train() if train else model.eval()\n#     total_loss, total_tokens = 0.0, 0\n#     for src, src_lens, tgt, _ in dataloader:\n#         src, src_lens, tgt = src.to(DEVICE), src_lens.to(DEVICE), tgt.to(DEVICE)\n#         if train:\n#             with torch.amp.autocast('cuda', enabled=(DEVICE=='cuda')):\n#                 logits = model(src, src_lens, tgt, teacher_forcing=0.8)\n#                 gold   = tgt[1:].contiguous().view(-1)\n#                 loss   = crit(logits.view(-1, logits.size(-1)), gold)\n#             optim.zero_grad(set_to_none=True)\n#             scaler.scale(loss).backward()\n#             scaler.unscale_(optim)\n#             nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n#             scaler.step(optim); scaler.update()\n#         else:\n#             with torch.no_grad(), torch.amp.autocast('cuda', enabled=(DEVICE=='cuda')):\n#                 logits = model(src, src_lens, tgt, teacher_forcing=1.0)\n#                 gold   = tgt[1:].contiguous().view(-1)\n#                 loss   = crit(logits.view(-1, logits.size(-1)), gold)\n#         ntoks = (gold != PAD).sum().item()\n#         total_loss += float(loss) * ntoks\n#         total_tokens += ntoks\n#     return total_loss / max(total_tokens, 1)\n\n# # ---------- continue training ----------\n# best_val = float(\"inf\")\n# for epoch in range(start_epoch, EPOCHS+1):\n#     t0 = time.time()\n#     tr_loss = run_epoch(dl_train, train=True)\n#     val_loss = run_epoch(dl_valid, train=False)\n#     dt = time.time() - t0\n#     print(f\"Epoch {epoch}: train_ppl={math.exp(tr_loss):.2f}  val_ppl={math.exp(val_loss):.2f}  time={dt/60:.1f}m\")\n\n#     torch.save({\"epoch\": epoch,\n#                 \"model\": model.state_dict(),\n#                 \"optim\": optim.state_dict(),\n#                 \"scaler\": scaler.state_dict()}, last_path)\n\n#     if val_loss < best_val:\n#         best_val = val_loss\n#         torch.save(model.state_dict(), best_path)\n#         print(\"Continue saving best checkpoint ->\", best_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# resume_path = \"/kaggle/working/seq2seq_last.pt\"\n# start_epoch = 1\n# if os.path.exists(resume_path):\n#     ckpt = torch.load(resume_path, map_location=DEVICE)\n#     model.load_state_dict(ckpt[\"model\"])\n#     optim.load_state_dict(ckpt[\"optim\"])\n#     try:\n#         scaler.load_state_dict(ckpt[\"scaler\"])\n#     except Exception:\n#         pass\n#     start_epoch = ckpt.get(\"epoch\", 0) + 1\n#     print(f\"Resuming from epoch {start_epoch} (loaded {resume_path})\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**first try:**","metadata":{}},{"cell_type":"code","source":"# reload best weights for inference\nmodel.load_state_dict(torch.load(\"/kaggle/working/seq2seq_best.pt\", map_location=DEVICE))\nmodel.eval()\n\ndef encode_src_batch(texts):\n    ids = [sp.EncodeAsIds(s)[:MAX_SRC_LEN-1] + [EOS] for s in texts]\n    lens = torch.tensor([len(x) for x in ids])\n    T = max(lens).item(); B = len(ids)\n    pad = torch.full((T,B), PAD, dtype=torch.long)\n    for i, seq in enumerate(ids):\n        pad[:len(seq), i] = torch.tensor(seq)\n    return pad.to(DEVICE), lens.to(DEVICE)\n\ndef decode_ids(ids):\n    if EOS in ids: ids = ids[:ids.index(EOS)]\n    if ids and ids[0]==BOS: ids = ids[1:]\n    return sp.DecodeIds(ids)\n\n@torch.no_grad()\ndef translate(texts, max_len=MAX_TGT_LEN):\n    if isinstance(texts, str): texts = [texts]\n    src, src_lens = encode_src_batch(texts)\n    gen = model.generate(src, src_lens, max_len=max_len)\n    return [decode_ids(gen[:,b].tolist()) for b in range(gen.size(1))]\n\nprint(translate([\"Tehran is the capital of Iran.\", \"How are you today?\"]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# reload best weights for inference\nmodel.load_state_dict(torch.load(\"/kaggle/working/seq2seq_best.pt\", map_location=DEVICE))\nmodel.eval()\n\ndef encode_src_batch(texts):\n    ids = [sp.EncodeAsIds(s)[:MAX_SRC_LEN-1] + [EOS] for s in texts]\n    lens = torch.tensor([len(x) for x in ids])\n    T = max(lens).item(); B = len(ids)\n    pad = torch.full((T,B), PAD, dtype=torch.long)\n    for i, seq in enumerate(ids):\n        pad[:len(seq), i] = torch.tensor(seq)\n    return pad.to(DEVICE), lens.to(DEVICE)\n\ndef decode_ids(ids):\n    if EOS in ids: ids = ids[:ids.index(EOS)]\n    if ids and ids[0]==BOS: ids = ids[1:]\n    return sp.DecodeIds(ids)\n\n@torch.no_grad()\ndef translate(texts, max_len=MAX_TGT_LEN):\n    if isinstance(texts, str): texts = [texts]\n    src, src_lens = encode_src_batch(texts)\n    gen = model.generate(src, src_lens, max_len=max_len)\n    return [decode_ids(gen[:,b].tolist()) for b in range(gen.size(1))]\n\nprint(translate([\"Tehran is the capital of Iran.\", \"How are you today?\"]))\nprint(translate([\"this is arefe.\", \"How are you today?\"]))\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}