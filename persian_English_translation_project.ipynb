{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "eb6719d6"
   },
   "source": [
    "# Persian-English Translator Project (Classical Seq2Seq)\n",
    "\n",
    "This notebook implements a classical Persian↔English translator using Seq2Seq (LSTM). It includes preprocessing, POS tagging, NER, tokenization, embedding, model design, training, and evaluation.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "38a0a6f3"
   },
   "source": [
    "## Table of contents\n",
    "1. Project goals\n",
    "2. Environment setup\n",
    "3. Data collection\n",
    "4. Preprocessing\n",
    "5. POS tagging\n",
    "6. NER\n",
    "7. Tokenization & Embedding\n",
    "8. Seq2Seq model\n",
    "9. Evaluation\n",
    "10. Analysis and report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "d2d98dff"
   },
   "source": [
    "## 2. Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GhPvpqmRpT5F",
    "outputId": "98a2b7b1-8eda-44cf-d61b-b7f649a633ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (25.2)\n",
      "Requirement already satisfied: numpy in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (2.3.2)\n",
      "Requirement already satisfied: matplotlib in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (3.10.6)\n",
      "Requirement already satisfied: scikit-learn in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (1.7.2)\n",
      "Requirement already satisfied: tensorflow in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (2.20.0)\n",
      "Requirement already satisfied: keras in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (3.11.3)\n",
      "Requirement already satisfied: hazm in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (0.9.3)\n",
      "Requirement already satisfied: parsivar in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (0.2.3.1)\n",
      "Requirement already satisfied: sacrebleu in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: nltk in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: sentencepiece in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (0.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from matplotlib) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from tensorflow) (6.32.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from tensorflow) (1.75.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: rich in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from keras) (14.1.0)\n",
      "Requirement already satisfied: namex in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from keras) (0.1.0)\n",
      "Requirement already satisfied: optree in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from keras) (0.17.0)\n",
      "Requirement already satisfied: fasttext-wheel<0.10.0,>=0.9.2 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from hazm) (0.9.2)\n",
      "Requirement already satisfied: gensim<5.0.0,>=4.3.1 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from hazm) (4.3.3)\n",
      "Requirement already satisfied: python-crfsuite<0.10.0,>=0.9.9 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from hazm) (0.9.11)\n",
      "Requirement already satisfied: click in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: pybind11>=2.2 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (3.0.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from gensim<5.0.0,>=4.3.1->hazm) (7.3.1)\n",
      "Requirement already satisfied: portalocker in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from sacrebleu) (6.0.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from rich->keras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from rich->keras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: datasets in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (4.1.1)\n",
      "Requirement already satisfied: transformers in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (4.56.2)\n",
      "Requirement already satisfied: seqeval in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (1.2.2)\n",
      "Requirement already satisfied: filelock in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from datasets) (0.35.1)\n",
      "Requirement already satisfied: packaging in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from seqeval) (1.7.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install numpy pandas matplotlib scikit-learn tensorflow keras hazm parsivar sacrebleu nltk sentencepiece\n",
    "# Optional: for advanced Persian NER datasets and models\n",
    "!pip install datasets transformers seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4",
   "metadata": {
    "id": "VsNUOscVlUya"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip install hazm\\n!pip install parsivar\\n!pip install sacrebleu\\n!pip install nltk\\n!pip install sentencepiece\\n!pip install datasets\\n!pip install transformers\\n!pip install seqeval\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "!pip install hazm\n",
    "!pip install parsivar\n",
    "!pip install sacrebleu\n",
    "!pip install nltk\n",
    "!pip install sentencepiece\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install seqeval\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9659ed9-7953-40c0-9556-a6d236b108f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cudf.pandas extension is not loaded.\n"
     ]
    }
   ],
   "source": [
    "%unload_ext cudf.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5",
   "metadata": {
    "id": "XByeLxawl6EB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-24 18:28:40.412122: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-24 18:28:40.419399: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-24 18:28:40.789652: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-24 18:28:42.579659: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-24 18:28:42.582040: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import hazm\n",
    "import parsivar\n",
    "import sacrebleu\n",
    "import nltk\n",
    "import sentencepiece\n",
    "import datasets\n",
    "import transformers\n",
    "import seqeval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "79TV9l_bU0d0"
   },
   "source": [
    "## 3. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "id": "fQQSIYqMVX_I"
   },
   "source": [
    "### 3.1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8",
   "metadata": {
    "id": "UQCFSNWXYRgQ"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import re, os\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "id": "WbmhaWaelC5T"
   },
   "source": [
    "### 3.2: Load dataset and inspect (normal load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GuM-dgZVU1Fa",
    "outputId": "9f4da60b-11d6-47e9-fbc6-567c71e764b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset (this may take a minute)...\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['flash fire .', 'فلاش آتش .'],\n",
      "        num_rows: 3960172\n",
      "    })\n",
      "})\n",
      "Split: train\n",
      "Columns: ['flash fire .', 'فلاش آتش .']\n",
      "\n",
      "One sample (0):\n",
      "{'flash fire .': 'superheats the air . burns the lungs like rice paper .',\n",
      " 'فلاش آتش .': 'هوا را فوق العاده گرم می کند . ریه ها را مثل کاغذ برنج می '\n",
      "               'سوزاند .'}\n"
     ]
    }
   ],
   "source": [
    "# Data collection: load the Hugging Face dataset (full, non-streaming)\n",
    "\n",
    "DATASET_ID = \"shenasa/English-Persian-Parallel-Dataset\"\n",
    "\n",
    "print(\"Loading dataset (this may take a minute)...\")\n",
    "ds = load_dataset(DATASET_ID)   # loads all splits (here likely only 'train')\n",
    "print(ds)                       # quick summary (splits, size)\n",
    "\n",
    "# Print column names and example\n",
    "split_name = list(ds.keys())[0]      # should be 'train'\n",
    "print(\"Split:\", split_name)\n",
    "print(\"Columns:\", ds[split_name].column_names)\n",
    "print(\"\\nOne sample (0):\")\n",
    "pprint(ds[split_name][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "w_j5f_ZEYxKU"
   },
   "source": [
    "## 4. Preprocessing (Normalization & cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "eIuV5c4vaSdT"
   },
   "source": [
    "### 4.1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13",
   "metadata": {
    "id": "OB1Mo1EmZRwX"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import re\n",
    "from hazm import Normalizer\n",
    "from datasets import DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "PDkEaYqTaPJa"
   },
   "source": [
    "### 4.2: Auto-detect which column is Persian / English & rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vlfky0FvVibB",
    "outputId": "e21d7fe0-e672-47c3-ba57-f09c7e7ef4a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Persian column: فلاش آتش .\n",
      "Detected English column: flash fire .\n"
     ]
    }
   ],
   "source": [
    "# Identify which column contains Persian text and which column contains English.\n",
    "def is_persian(s):\n",
    "    if s is None: return False\n",
    "    return bool(re.search(r'[\\u0600-\\u06FF]', s))\n",
    "\n",
    "cols = ds[split_name].column_names\n",
    "sample = ds[split_name][0]\n",
    "\n",
    "persian_col = None\n",
    "english_col = None\n",
    "for c in cols:\n",
    "    try:\n",
    "        if is_persian(sample[c]):\n",
    "            persian_col = c\n",
    "        else:\n",
    "            english_col = c\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"Detected Persian column:\", persian_col)\n",
    "print(\"Detected English column:\", english_col)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "qhKse3AGaH7v"
   },
   "source": [
    "### 4.3: Rename columns to standard names and run light cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "fe6ef1a5187a41d1baf2b97900bf64d7",
      "a3c3068d13ef431e900948ed009e6057",
      "8cc306a9ba3c4376911324100ca34f99",
      "661a3739188d4e98a07b67a7467dcb94",
      "c0157f93e6cc4a8d8868dc6c58f107b4",
      "14858554626c43d8a0ee0530f7a9799f",
      "f6ec497f901e4278aefcdbea1a91b23e",
      "f859fb89f06a4296a98f283fcd7d443b",
      "fd168d2dcbf34bec974775a2e99fc2a2",
      "4bad3c837409477cb8990ced874d9bba",
      "dcedf573589a45bb8a3a58c98bfec832"
     ]
    },
    "id": "vI8KhTDEYLEe",
    "outputId": "49fddc41-2baa-44c2-fb1d-b179fd5e1b05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping and renaming columns... (this may take a while for large datasets)\n",
      "New columns: ['persian', 'english']\n",
      "Sample: {'persian': 'هوا را فوق العاده گرم می کند . ریه ها را مثل کاغذ برنج می سوزاند .', 'english': 'superheats the air . burns the lungs like rice paper .'}\n"
     ]
    }
   ],
   "source": [
    "# If the dataset columns are e.g. 'translation' you may need to adapt the mapping below.\n",
    "\n",
    "def rename_and_select(example):\n",
    "    return {\"persian\": example[persian_col], \"english\": example[english_col]}\n",
    "\n",
    "print(\"Mapping and renaming columns... (this may take a while for large datasets)\")\n",
    "ds_simple = ds[split_name].map(rename_and_select, remove_columns=ds[split_name].column_names)\n",
    "\n",
    "print(\"New columns:\", ds_simple.column_names)\n",
    "print(\"Sample:\", ds_simple[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "2JRJB26MaCyl"
   },
   "source": [
    "### 4.4: Cleaning helpers (Persian normalizer + English lite cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRTWAQKAY6Ve",
    "outputId": "0a98fa3f-5ff1-44f8-f7f4-1dd75c51efe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "این 123 کتاب هاست‎\n",
      "this is a test!\n"
     ]
    }
   ],
   "source": [
    "# This is a robust, minimal cleaning pipeline for Persian + basic English normalization.\n",
    "\n",
    "normalizer = Normalizer()\n",
    "\n",
    "AR_TO_FA = {'\\u064A': '\\u06CC', '\\u0643': '\\u06A9'}\n",
    "ZERO_WIDTH = ['\\u200c', '\\u200f', '\\u202a', '\\u202b']\n",
    "PERSIAN_DIGITS = '۰۱۲۳۴۵۶۷۸۹'\n",
    "ASCII_DIGITS = '0123456789'\n",
    "\n",
    "def replace_arabic_chars(text):\n",
    "    for a,f in AR_TO_FA.items(): text = text.replace(a,f)\n",
    "    return text\n",
    "\n",
    "def remove_zero_width(text):\n",
    "    for ch in ZERO_WIDTH: text = text.replace(ch,'')\n",
    "    return text\n",
    "\n",
    "def persian_to_ascii_digits(text):\n",
    "    for p,a in zip(PERSIAN_DIGITS, ASCII_DIGITS): text = text.replace(p,a)\n",
    "    return text\n",
    "\n",
    "def clean_persian(text):\n",
    "    if text is None: return \"\"\n",
    "    text = str(text)\n",
    "    text = replace_arabic_chars(text)\n",
    "    text = remove_zero_width(text)\n",
    "    try:\n",
    "        text = normalizer.normalize(text)\n",
    "    except Exception:\n",
    "        pass\n",
    "    text = persian_to_ascii_digits(text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def clean_english(text):\n",
    "    if text is None: return \"\"\n",
    "    text = str(text).strip()\n",
    "    # optional: lowercasing (depends if you want to preserve casing)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# Quick test\n",
    "print(clean_persian(\"این ۱۲۳ کتاب‌ هاست‎\"))\n",
    "print(clean_english(\" This IS  a Test! \"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "id": "8mN-koSPZ8pD"
   },
   "source": [
    "### 4.5: Apply cleaning (batched) and filter empty/very-long pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153,
     "referenced_widgets": [
      "d35a3bc0448c4e2e8e1a0bd03e85b5bd",
      "f8204c413a5d4940bab769bcf1b145bc",
      "d8524c0c25c24ff9a94ce553a59e3b1d",
      "d8830f5b1e734d979383cf661fc8765c",
      "22125cfa033e44af992ba575eebde135",
      "eebf6946657a4f23a6b52766ca282364",
      "3c8c1a91896c48abb2b69083a06e9f91",
      "34e100915ad64579a6ec03a7bb9ee529",
      "55e181cc08d7461fb55038f314a9cae1",
      "7653560a97a745078b9b5ed9e6f4d15b",
      "5be5f4b9b85541b19543bd7bb352476f",
      "ca0d3fa7c1234eb3b3bd1bbe812780b5",
      "a3d41537d15a401c86ae1ad269141e0f",
      "c1fbd193df754c4ba5eb182b0a470fde",
      "2483868f72984b95859faf39a9e2875f",
      "738c6a63cfc74da7bfe691415e01d643",
      "b88352d9bf2f4373b1593c41b9f7b505",
      "c7420a60cada475da61843ff307aad2f",
      "2439eff6d3b341c6b80573bf081c9fbd",
      "4a358eaae61746b190d32f0e52d07c6f",
      "4e09eccefadc4eb1be40d2c272132a16",
      "99e77a47bef14aa784a06af38e7f28de"
     ]
    },
    "id": "xjGPDs0EZwfO",
    "outputId": "6d8ca2a7-ace9-41f7-a5cf-b99e65d159f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning dataset (batched)...\n",
      "After cleaning sample: {'persian': 'هوا را فوق\\u200cالعاده گرم می\\u200cکند. ریه\\u200cها را مثل کاغذ برنج می\\u200cسوزاند.', 'english': 'superheats the air . burns the lungs like rice paper .'}\n",
      "Size after filter: 3948961\n"
     ]
    }
   ],
   "source": [
    "# Apply cleaning to dataset (use num_proc if Colab CPU allows parallelism)\n",
    "def clean_example(ex):\n",
    "    return {\n",
    "        \"persian\": clean_persian(ex[\"persian\"]),\n",
    "        \"english\": clean_english(ex[\"english\"])\n",
    "    }\n",
    "\n",
    "print(\"Cleaning dataset (batched)...\")\n",
    "ds_clean = ds_simple.map(clean_example, batched=False)  # batched=True can be faster but needs function change\n",
    "print(\"After cleaning sample:\", ds_clean[0])\n",
    "\n",
    "# Filter out empty or too-long sentences (token-length heuristics)\n",
    "MAX_CHARS = 512\n",
    "def filter_empty_or_long(ex):\n",
    "    if ex[\"persian\"].strip()==\"\" or ex[\"english\"].strip()==\"\":\n",
    "        return False\n",
    "    if len(ex[\"persian\"]) > MAX_CHARS or len(ex[\"english\"]) > MAX_CHARS:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "ds_clean = ds_clean.filter(filter_empty_or_long)\n",
    "print(\"Size after filter:\", len(ds_clean))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "tQe3XlzNazuD"
   },
   "source": [
    "### 4.6: Create train/validation/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GwdmQ9Nea4Y2",
    "outputId": "ed8f3c3a-ae35-4f51-c9ed-4f00853bfc4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 3948961\n",
      "train 3830492\n",
      "validation 59234\n",
      "test 59235\n",
      "Example pair (train[0]):\n",
      "{'persian': 'Thakhek 5121 **** تلفن', 'english': 'thakhek 5121 **** phone'}\n"
     ]
    }
   ],
   "source": [
    "# If the original dataset already had splits, you can directly use them.\n",
    "# Here we assume ds_clean is a single large 'train' and we want to create val/test.\n",
    "total = len(ds_clean)\n",
    "print(\"Total pairs:\", total)\n",
    "\n",
    "if total > 1000:\n",
    "    # create ~3% holdout; then split that half for val/test → ~1.5% each\n",
    "    split1 = ds_clean.train_test_split(test_size=0.03, seed=42, shuffle=True)\n",
    "    hold = split1['test'].train_test_split(test_size=0.5, seed=42)\n",
    "    dataset_splits = DatasetDict({\n",
    "        'train': split1['train'],\n",
    "        'validation': hold['train'],\n",
    "        'test': hold['test']\n",
    "    })\n",
    "else:\n",
    "    # small dataset: create 80/10/10\n",
    "    split1 = ds_clean.train_test_split(test_size=0.2, seed=42)\n",
    "    hold = split1['test'].train_test_split(test_size=0.5, seed=42)\n",
    "    dataset_splits = DatasetDict({\n",
    "        'train': split1['train'],\n",
    "        'validation': hold['train'],\n",
    "        'test': hold['test']\n",
    "    })\n",
    "\n",
    "for k in dataset_splits:\n",
    "    print(k, len(dataset_splits[k]))\n",
    "\n",
    "# Peek at a few examples\n",
    "print(\"Example pair (train[0]):\")\n",
    "print(dataset_splits['train'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "id": "nIRHmhm7bS6U"
   },
   "source": [
    "### 4.7: Save to disk / optional Google Drive mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130,
     "referenced_widgets": [
      "a3dfb8aca7904f7a9207ea7fb864e625",
      "72423bf8e7764de5b4f813308f3f154e",
      "98b6bd9e7a634d59b3fa971c65f41566",
      "cc94c36fe8d4415985ebbd00ace74796",
      "7907856f5b434fd997f12edad5e054e2",
      "88f8f66a83fd497ca79eb9533100b9ea",
      "26865c6faff345a08c8efd1b798b415a",
      "16e8d6b0b1db408c80f782fa69e4fb56",
      "896e4fce0ea64cfbac66e14173dfe221",
      "a001e8f6cbbb4772abeaa4f477d6e355",
      "b68d9592f28c41e4b0b8b9e2a66eb2e1",
      "35950ae55ca145d3856a877df53ff0ed",
      "ff9f59d39ec74eee9bbac66810575122",
      "692a96b3db1d4fe89008d6ddc6bc68ce",
      "ba47dfce00ac415986ea607c138836b1",
      "11767bb1d33943a8a70ba562a115660f",
      "04571fc447774083a7f3b86d77c4d1e1",
      "5de0fe911b9d4c2c91f3ae52d2e35799",
      "18ca0492bd634fd690a97815ff3ba18b",
      "239dc46e10a549eba19b475f947cf7e2",
      "8f5b37443243433cb92cdac50e7b90f8",
      "185ca13c7cb44aefaaf5e9951cf8d123",
      "6daa7e7af8b6459daea21dffcdbe1006",
      "aa0866870a5c49428d935be6b20bc369",
      "75dd63288e3a46a9bceb406185e1b91a",
      "1867a084f09b4d41a213fc2570ba17ef",
      "f49c0865ff724f7fa5a5d197aa142402",
      "04534b8905af4670aca9a42b550c21fd",
      "b4576bef80a048e59aed60d4d60a1af5",
      "c9985296ca1645f9bca64f3e762c2e2c",
      "399f4270d1794a7cb486fa77a25f4dcd",
      "01bece9017e746fabfb951a61db8747c",
      "3faacd215dd34f689f897131d62efd27"
     ]
    },
    "id": "Udn4bc_Xa4A5",
    "outputId": "b7050477-445f-4864-d122-01f18451ba18"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e804547637724b7c86bb6f535b05cb3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/3831 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683aa5f6e063425b94d190a5d6f34600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/60 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873b95b2ba664e0c98512d6615318b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/60 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CSVs to ./data/\n"
     ]
    }
   ],
   "source": [
    "# Option A: save to Colab local storage\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "dataset_splits['train'].to_csv(\"data/train.csv\")\n",
    "dataset_splits['validation'].to_csv(\"data/validation.csv\")\n",
    "dataset_splits['test'].to_csv(\"data/test.csv\")\n",
    "print(\"Saved CSVs to ./data/\")\n",
    "\n",
    "# Option B: save to Google Drive (uncomment to use)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# out_dir = \"/content/drive/MyDrive/persian_translation_dataset\"\n",
    "# os.makedirs(out_dir, exist_ok=True)\n",
    "# dataset_splits['train'].to_csv(os.path.join(out_dir,\"train.csv\"))\n",
    "# dataset_splits['validation'].to_csv(os.path.join(out_dir,\"validation.csv\"))\n",
    "# dataset_splits['test'].to_csv(os.path.join(out_dir,\"test.csv\"))\n",
    "# print(\"Saved CSVs to Google Drive:\", out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "id": "2x4EnvMHbdKR"
   },
   "source": [
    "### 4.8: Create a small subset for fast experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "240e27e1a4b44442962b009edee676c3",
      "7507c6adadce47db95d1a0af6c01d60d",
      "0f79c9eb70514743a900e412cff76fa9",
      "03c4c3b9489d4c6b8802bac7212e7fc3",
      "646e34d3251c4361bea099c4b17cc135",
      "408b6ea8ca42494ea8b744504933a03f",
      "6d8fc420dcb444d487dbc677030e2584",
      "aa00dd6b425e42b9b36ce66227c71ed6",
      "7d7cd1843a2945bc8aa9c817e904a2f9",
      "5bb7a2846b4c4fbb83d8f563155c27da",
      "2523f507c88840808bc80847f4546755"
     ]
    },
    "id": "F0t4I_Kbb8pQ",
    "outputId": "c4d67897-a61a-4d3e-9d36-827619087845"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8867af559fab486cab94edd32bbc311f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved small train: 10000\n"
     ]
    }
   ],
   "source": [
    "# Create a small subset (e.g., 10k pairs) for fast development / debugging.\n",
    "small_size = 10000\n",
    "if len(dataset_splits['train']) > small_size:\n",
    "    small_train = dataset_splits['train'].select(range(small_size))\n",
    "else:\n",
    "    small_train = dataset_splits['train']\n",
    "\n",
    "small_train.to_csv(\"data/train_small.csv\")\n",
    "print(\"Saved small train:\", len(small_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "id": "3h5KruwMdPJb"
   },
   "source": [
    "### 4.9: Streaming mode — inspect a few examples without full download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5SG0sozqdUDB",
    "outputId": "9ce2869a-20f3-4744-91f3-d66cd38516b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming a few examples (no full download):\n",
      "0 {'flash fire .': 'superheats the air . burns the lungs like rice paper .', 'فلاش آتش .': 'هوا را فوق العاده گرم می کند . ریه ها را مثل کاغذ برنج می سوزاند .'}\n",
      "1 {'flash fire .': 'hey , guys . down here . down here .', 'فلاش آتش .': 'سلام بچه ها . این پایین . این پایین .'}\n",
      "2 {'flash fire .': 'what do you got down this corridor is the bow , right .', 'فلاش آتش .': 'چه چیزی در این راهرو پایین آمده است ، درست است .'}\n",
      "3 {'flash fire .': 'theres an access hatch right there that puts us into the bowthruster room .', 'فلاش آتش .': 'یک دریچه دسترسی درست در آنجا وجود دارد که ما را وارد اتاق کمان می کند .'}\n",
      "4 {'flash fire .': 'we get into the propeller tubes and the only thing between us and the outside .', 'فلاش آتش .': 'وارد لوله های پروانه می شویم و تنها چیزی که بین ما و بیرون است .'}\n",
      "5 {'flash fire .': 'is nothing . all right . lets go .', 'فلاش آتش .': 'هیچی نیست . خیلی خوب . بیا بریم .'}\n",
      "6 {'flash fire .': 'lets go . thats our way out .', 'فلاش آتش .': 'بیا بریم . این راه خروج ماست'}\n",
      "7 {'flash fire .': 'all the way to the end , and up the hatch .', 'فلاش آتش .': 'تمام راه تا انتها و بالا دریچه .'}\n",
      "8 {'flash fire .': 'wait a minute . wait a minute .', 'فلاش آتش .': 'یک دقیقه صبر کن . یک دقیقه صبر کن .'}\n",
      "9 {'flash fire .': 'bows underwater . what does that mean .', 'فلاش آتش .': 'زیر آب تعظیم می کند . معنی آن چیست .'}\n",
      "10 {'flash fire .': 'the thrusters are under there . how far maybe we can swim .', 'فلاش آتش .': 'رانشگرها در زیر هستند . تا کجا شاید بتوانیم شنا کنیم .'}\n"
     ]
    }
   ],
   "source": [
    "# If dataset is extremely large and you prefer to stream:\n",
    "print(\"Streaming a few examples (no full download):\")\n",
    "stream_ds = load_dataset(DATASET_ID, split=\"train\", streaming=True)\n",
    "for i, ex in enumerate(stream_ds):\n",
    "    print(i, ex)\n",
    "    if i >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "id": "cdf03190"
   },
   "source": [
    "## 5. POS tagging (Hazm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31",
   "metadata": {
    "id": "e3GpeSIvjtMC"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a79bbf47a94fdbbffbb949db434665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/60 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/validation_with_pos.csv rows: 59234\n",
      " ...\n",
      " ...\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def add_pos_columns(ds_split):\n",
    "    # Persian (Parsivar → Hazm fallback)\n",
    "    fa_backend = None\n",
    "    fa_tokenizer = None\n",
    "    fa_tagger = None\n",
    "\n",
    "    try:\n",
    "        from parsivar import Normalizer as PVNormalizer, Tokenizer as PVTokenizer, POSTagger as PVPOSTagger\n",
    "        fa_norm_pv = PVNormalizer()\n",
    "        fa_tokenizer = PVTokenizer()\n",
    "        fa_tagger = PVPOSTagger(tagging_model=\"wapiti\")\n",
    "        fa_backend = \"parsivar\"\n",
    "    except Exception:\n",
    "        try:\n",
    "            from hazm import POSTagger as HZPOSTagger, word_tokenize as hz_word_tokenize\n",
    "            fa_tagger = HZPOSTagger()\n",
    "            fa_backend = \"hazm\"\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    import nltk\n",
    "    from nltk import pos_tag as nltk_pos_tag, word_tokenize as nltk_word_tokenize\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        try: nltk.download('punkt', quiet=True)\n",
    "        except: pass\n",
    "    try:\n",
    "        nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "    except LookupError:\n",
    "        try: nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "        except: pass\n",
    "\n",
    "    def pos_str(pairs: List[Tuple[str,str]]) -> str:\n",
    "        return \" \".join(f\"{w}/{t}\" for w,t in pairs)\n",
    "\n",
    "    def tag_fa(text: str) -> str:\n",
    "        if not text or fa_tagger is None: return \"\"\n",
    "        try:\n",
    "            if fa_backend == \"parsivar\":\n",
    "                t = fa_norm_pv.normalize(text)\n",
    "                toks = fa_tokenizer.tokenize_words(t)\n",
    "                tags = fa_tagger.tag(toks)\n",
    "                return pos_str(list(zip(toks, tags)))\n",
    "            else:\n",
    "                toks = hz_word_tokenize(text)\n",
    "                return pos_str(fa_tagger.tag(toks))\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "    def tag_en(text: str) -> str:\n",
    "        if not text: return \"\"\n",
    "        try:\n",
    "            toks = nltk_word_tokenize(text)\n",
    "            return pos_str(nltk_pos_tag(toks))\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "    def add_pos(batch):\n",
    "        return {\n",
    "            \"fa_pos\": [tag_fa(x) for x in batch[\"persian\"]],\n",
    "            \"en_pos\": [tag_en(x) for x in batch[\"english\"]],\n",
    "        }\n",
    "\n",
    "    ds_with_pos = ds_split.map(add_pos, batched=True, batch_size=64)\n",
    "    return ds_with_pos\n",
    "\n",
    "# Example:\n",
    "pos_valid = add_pos_columns(dataset_splits[\"validation\"])\n",
    "pos_valid.to_csv(\"data/validation_with_pos.csv\")\n",
    "print(\"Saved:\", \"data/validation_with_pos.csv\", \"rows:\", len(pos_valid))\n",
    "print(pos_valid[0][\"fa_pos\"][:120], \"...\")\n",
    "print(pos_valid[0][\"en_pos\"][:120], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {
    "id": "e76f950a"
   },
   "source": [
    "## 6. Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33",
   "metadata": {
    "id": "guOoFo1_vdAB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Downloading stanza-1.10.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: parsivar in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (0.2.3.1)\n",
      "Collecting emoji (from stanza)\n",
      "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: numpy in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from stanza) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from stanza) (6.32.1)\n",
      "Requirement already satisfied: requests in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from stanza) (2.32.5)\n",
      "Collecting networkx (from stanza)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting torch>=1.3.0 (from stanza)\n",
      "  Using cached torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: tqdm in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from stanza) (4.67.1)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.13-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.10-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.19.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Using cached pydantic-2.11.9-py3-none-any.whl.metadata (68 kB)\n",
      "Requirement already satisfied: jinja2 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from spacy) (25.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from requests->stanza) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from requests->stanza) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from requests->stanza) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from requests->stanza) (2025.8.3)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy (from stanza)\n",
      "  Using cached numpy-2.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.1.0)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.22.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
      "Requirement already satisfied: wrapt in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
      "Requirement already satisfied: nltk>=3.6.6 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from parsivar) (3.9.1)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.3.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: joblib in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from nltk>=3.6.6->parsivar) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from nltk>=3.6.6->parsivar) (2025.9.18)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: filelock in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from torch>=1.3.0->stanza) (3.19.1)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.3.0->stanza)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: fsspec in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from torch>=1.3.0->stanza) (2025.9.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=1.3.0->stanza)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=1.3.0->stanza)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=1.3.0->stanza)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=1.3.0->stanza)\n",
      "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=1.3.0->stanza)\n",
      "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=1.3.0->stanza)\n",
      "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=1.3.0->stanza)\n",
      "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=1.3.0->stanza)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=1.3.0->stanza)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch>=1.3.0->stanza)\n",
      "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch>=1.3.0->stanza)\n",
      "  Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=1.3.0->stanza)\n",
      "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=1.3.0->stanza)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=1.3.0->stanza)\n",
      "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.4.0 (from torch>=1.3.0->stanza)\n",
      "  Using cached triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.3.0->stanza)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Downloading stanza-1.10.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m58.8 kB/s\u001b[0m  \u001b[33m0:00:21\u001b[0m \u001b[31m60.4 kB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\n",
      "\u001b[?25hDownloading spacy-3.8.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.9/33.9 MB\u001b[0m \u001b[31m337.9 kB/s\u001b[0m  \u001b[33m0:03:20\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:03\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (227 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.13-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
      "Downloading preshed-3.0.10-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (869 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.3/869.3 kB\u001b[0m \u001b[31m521.4 kB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m.2 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.11.9-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m523.0 kB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m\u001b[31m629.5 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m409.9 kB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m\u001b[31m444.8 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.3.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m409.7 kB/s\u001b[0m  \u001b[33m0:00:10\u001b[0m\u001b[31m393.5 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading blis-1.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m884.6 kB/s\u001b[0m  \u001b[33m0:00:13\u001b[0m\u001b[31m889.0 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached numpy-2.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "Downloading typer-0.19.2-py3-none-any.whl (46 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.22.0-py3-none-any.whl (61 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m448.9 kB/s\u001b[0m  \u001b[33m0:00:11\u001b[0m\u001b[31m445.6 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading marisa_trie-1.3.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m722.1 kB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m7.0 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n",
      "Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m664.6 kB/s\u001b[0m  \u001b[33m0:15:41\u001b[0m[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:45\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m687.0 kB/s\u001b[0m  \u001b[33m0:05:08\u001b[0m[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:08\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m666.3 kB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m8.0 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m505.6 kB/s\u001b[0m  \u001b[33m0:02:13\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:04\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m661.6 kB/s\u001b[0m  \u001b[33m0:09:26\u001b[0m[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:14\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m159.4 kB/s\u001b[0m  \u001b[33m0:09:22\u001b[0m[0m eta \u001b[36m0:00:01\u001b[0m[36m0:01:42\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m  \u001b[33m0:05:37\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:06\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m  \u001b[33m0:03:52\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:05\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m  \u001b[33m0:00:28\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m  \u001b[33m0:01:29\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, mpmath, cymem, wasabi, typing-inspection, triton, sympy, spacy-loggers, spacy-legacy, shellingham, pydantic-core, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, murmurhash, marisa-trie, emoji, cloudpathlib, catalogue, annotated-types, srsly, pydantic, preshed, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, language-data, blis, typer, nvidia-cusolver-cu12, langcodes, confection, weasel, torch, thinc, stanza, spacy\n",
      "\u001b[2K  Attempting uninstall: numpy━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/45\u001b[0m [nvidia-cublas-cu12]12]\n",
      "\u001b[2K    Found existing installation: numpy 1.26.449;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/45\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Uninstalling numpy-1.26.4:━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/45\u001b[0m [numpy]las-cu12]\n",
      "\u001b[2K      Successfully uninstalled numpy-1.26.42;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/45\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45/45\u001b[0m [spacy]/45\u001b[0m [spacy]/45\u001b[0m [stanza]nvidia-cusolver-cu12]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "hazm 0.9.3 requires numpy<2.0.0,>=1.24.3, but you have numpy 2.3.3 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.3 which is incompatible.\n",
      "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.7.0 blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.22.0 confection-0.1.5 cymem-2.0.11 emoji-2.15.0 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.3.1 mpmath-1.3.0 murmurhash-1.0.13 networkx-3.5 numpy-2.3.3 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 preshed-3.0.10 pydantic-2.11.9 pydantic-core-2.33.2 shellingham-1.5.4 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 stanza-1.10.1 sympy-1.14.0 thinc-8.3.6 torch-2.8.0 triton-3.4.0 typer-0.19.2 typing-inspection-0.4.1 wasabi-1.1.3 weasel-0.4.1\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m  \u001b[33m0:00:10\u001b[0ma \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# فقط اگر قبلاً نصب نیست\n",
    "!pip install stanza spacy  parsivar\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7abca70d-c5f7-45a2-808d-cf70763dd64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59234"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TARGET_SPLIT = \"validation\"   \n",
    "OUT_NER_CSV  = f\"data/{TARGET_SPLIT}_with_ner.csv\"\n",
    "\n",
    "import os\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "split = dataset_splits[TARGET_SPLIT]\n",
    "len(split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fec31a3-9adf-4f37-b1fa-7dc492b43ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Persian NER...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arefe/Downloads/NLP/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:182: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 302: Error loading CUDA libraries. GPU will not be used. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FA NER → stanza\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing Persian NER...\")\n",
    "fa_backend = None\n",
    "fa_stanza = None\n",
    "fa_pv_tok = None\n",
    "fa_pv_ner = None\n",
    "\n",
    "# ترجیح: stanza\n",
    "try:\n",
    "    import stanza\n",
    "    try:\n",
    "        # اگر مدل‌ها نیستند سعی می‌کند دانلود کند\n",
    "        stanza.Pipeline(lang='fa', processors='tokenize,ner', tokenize_no_ssplit=True, verbose=False)\n",
    "    except Exception:\n",
    "        try:\n",
    "            stanza.download('fa', processors='tokenize,ner', quiet=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "    fa_stanza = stanza.Pipeline(lang='fa', processors='tokenize,ner', tokenize_no_ssplit=True, verbose=False)\n",
    "    fa_backend = \"stanza\"\n",
    "    print(\"FA NER → stanza\")\n",
    "except Exception as e:\n",
    "    print(\"stanza not available:\", e)\n",
    "\n",
    "#Fallback: Parsivar\n",
    "if fa_stanza is None:\n",
    "    try:\n",
    "        from parsivar import Tokenizer as PVTok, NERTagger as PVNERTagger\n",
    "        fa_pv_tok = PVTok()\n",
    "        fa_pv_ner = PVNERTagger()\n",
    "        fa_backend = \"parsivar\"\n",
    "        print(\"FA NER → Parsivar\")\n",
    "    except Exception as e:\n",
    "        print(\"Parsivar NER not available:\", e)\n",
    "\n",
    "def ner_fa(text: str) -> str:\n",
    "    \"\"\"خروجی:\n",
    "       - stanza: 'متن|TYPE; ...'\n",
    "       - parsivar: 'tok/TAG tok/TAG ...'\n",
    "    \"\"\"\n",
    "    if not text: return \"\"\n",
    "    try:\n",
    "        if fa_backend == \"stanza\" and fa_stanza is not None:\n",
    "            doc = fa_stanza(text)\n",
    "            return \"; \".join(f\"{ent.text}|{ent.type}\" for ent in doc.entities)\n",
    "        elif fa_backend == \"parsivar\" and fa_pv_tok is not None and fa_pv_ner is not None:\n",
    "            toks = fa_pv_tok.tokenize_words(text)\n",
    "            tags = fa_pv_ner.tag(toks)\n",
    "            return \" \".join(f\"{w}/{t}\" for w,t in zip(toks, tags))\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d82ddacc-52a7-4fd6-b0b7-c0834db2596d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing English NER...\n",
      "EN NER → spaCy\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing English NER...\")\n",
    "en_backend = None\n",
    "en_spacy = None\n",
    "\n",
    "# ترجیح: spaCy\n",
    "try:\n",
    "    import spacy\n",
    "    try:\n",
    "        en_spacy = spacy.load(\"en_core_web_sm\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            from spacy.cli import download as spacy_download\n",
    "            spacy_download(\"en_core_web_sm\")\n",
    "            en_spacy = spacy.load(\"en_core_web_sm\")\n",
    "        except Exception:\n",
    "            en_spacy = None\n",
    "    if en_spacy is not None:\n",
    "        en_backend = \"spacy\"\n",
    "        print(\"EN NER → spaCy\")\n",
    "except Exception as e:\n",
    "    print(\"spaCy not available:\", e)\n",
    "\n",
    "#Fallback: NLTK\n",
    "if en_spacy is None:\n",
    "    try:\n",
    "        import nltk\n",
    "        from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "        for pkg in ['punkt','averaged_perceptron_tagger','maxent_ne_chunker','words']:\n",
    "            try:\n",
    "                nltk.data.find(nltk.downloader.Downloader()._packages[pkg].subdir + \"/\" + pkg)\n",
    "            except LookupError:\n",
    "                try: nltk.download(pkg, quiet=True)\n",
    "                except Exception: pass\n",
    "        en_backend = \"nltk\"\n",
    "        print(\"EN NER → NLTK\")\n",
    "    except Exception as e:\n",
    "        print(\"NLTK ne_chunk not available:\", e)\n",
    "\n",
    "def ner_en(text: str) -> str:\n",
    "    \"\"\"خروجی: 'TEXT|LABEL; ...'\"\"\"\n",
    "    if not text: return \"\"\n",
    "    try:\n",
    "        if en_backend == \"spacy\" and en_spacy is not None:\n",
    "            doc = en_spacy(text)\n",
    "            return \"; \".join(f\"{ent.text}|{ent.label_}\" for ent in doc.ents)\n",
    "        else:\n",
    "            import nltk\n",
    "            toks = nltk.word_tokenize(text)\n",
    "            pos  = nltk.pos_tag(toks)\n",
    "            tree = nltk.ne_chunk(pos, binary=False)\n",
    "            spans = []\n",
    "            for node in tree:\n",
    "                if hasattr(node, 'label'):\n",
    "                    words = \" \".join(leaf[0] for leaf in node.leaves())\n",
    "                    spans.append(f\"{words}|{node.label()}\")\n",
    "            return \"; \".join(spans)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46464fea-84c8-4890-ab15-d1d6accc3294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging NER on split: validation (size=59234)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a4f5af2089a43cea6defb4e29f8c14b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/59234 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d72b725a2464311945c83f00b929756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/60 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/validation_with_ner.csv rows: 59234\n",
      "{'persian': 'فقط با مشکلی دست و پنجه نرم می\\u200cکنم که به نظر می\\u200cرسد نمی\\u200cتوانم GU - 50 s را به سوکت فشار دهم.', 'english': 'just battling the problem that i seem not to be able to push the gu-50s into the sockets.', 'fa_ner': '', 'en_ner': ''}\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(f\"Tagging NER on split: {TARGET_SPLIT} (size={len(split)})\")\n",
    "\n",
    "def add_ner(batch):\n",
    "    fa_list, en_list = [], []\n",
    "    for fa, en in zip(batch[\"persian\"], batch[\"english\"]):\n",
    "        fa_list.append(ner_fa(fa))\n",
    "        en_list.append(ner_en(en))\n",
    "    return {\"fa_ner\": fa_list, \"en_ner\": en_list}\n",
    "\n",
    "# map به‌صورت بچ‌—تعادلی بین سرعت/حافظه\n",
    "ner_split = split.map(add_ner, batched=True, batch_size=32)\n",
    "\n",
    "ner_split.to_csv(OUT_NER_CSV)\n",
    "print(f\"Saved: {OUT_NER_CSV} rows: {len(ner_split)}\")\n",
    "\n",
    "# نمونهٔ سریع\n",
    "sample = ner_split[0]\n",
    "print({k: sample.get(k,\"\")[:180] for k in [\"persian\",\"english\",\"fa_ner\",\"en_ner\"]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "id": "b4a8ab1b"
   },
   "source": [
    "## 7. Tokenization & embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {
    "id": "c7a99d07"
   },
   "source": [
    "## 8. Seq2Seq model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {
    "id": "a9640856"
   },
   "source": [
    "## 9. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {
    "id": "dbb8ed52"
   },
   "source": [
    "## End of Notebook\n",
    "This Colab notebook covers preprocessing, POS tagging, NER, tokenization, Seq2Seq model, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "id": "aAtpXXIMxZNQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
