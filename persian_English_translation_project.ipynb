{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eb6719d6",
      "metadata": {
        "id": "eb6719d6"
      },
      "source": [
        "# Persian-English Translator Project (Classical Seq2Seq)\n",
        "\n",
        "This notebook implements a classical Persian↔English translator using Seq2Seq (LSTM). It includes preprocessing, POS tagging, NER, tokenization, embedding, model design, training, and evaluation.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38a0a6f3",
      "metadata": {
        "id": "38a0a6f3"
      },
      "source": [
        "## Table of contents\n",
        "1. Project goals\n",
        "2. Environment setup\n",
        "3. Data collection\n",
        "4. Preprocessing\n",
        "5. POS tagging\n",
        "6. NER\n",
        "7. Tokenization & Embedding\n",
        "8. Seq2Seq model\n",
        "9. Evaluation\n",
        "10. Analysis and report\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2d98dff",
      "metadata": {
        "id": "d2d98dff"
      },
      "source": [
        "## 2. Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install numpy pandas matplotlib scikit-learn tensorflow keras hazm parsivar sacrebleu nltk sentencepiece\n",
        "# Optional: for advanced Persian NER datasets and models\n",
        "!pip install datasets transformers seqeval"
      ],
      "metadata": {
        "id": "GhPvpqmRpT5F",
        "collapsed": true
      },
      "id": "GhPvpqmRpT5F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%load_ext cudf.pandas"
      ],
      "metadata": {
        "id": "dfGqwImhJAVY"
      },
      "id": "dfGqwImhJAVY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "import hazm\n",
        "import parsivar\n",
        "import sacrebleu\n",
        "import nltk\n",
        "import sentencepiece\n",
        "import datasets\n",
        "import transformers\n",
        "import seqeval"
      ],
      "metadata": {
        "id": "XByeLxawl6EB"
      },
      "id": "XByeLxawl6EB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data collection"
      ],
      "metadata": {
        "id": "79TV9l_bU0d0"
      },
      "id": "79TV9l_bU0d0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1: Imports"
      ],
      "metadata": {
        "id": "fQQSIYqMVX_I"
      },
      "id": "fQQSIYqMVX_I"
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "import re, os\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "UQCFSNWXYRgQ"
      },
      "id": "UQCFSNWXYRgQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2: Load dataset and inspect (normal load)"
      ],
      "metadata": {
        "id": "WbmhaWaelC5T"
      },
      "id": "WbmhaWaelC5T"
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collection: load the Hugging Face dataset (full, non-streaming)\n",
        "\n",
        "DATASET_ID = \"shenasa/English-Persian-Parallel-Dataset\"\n",
        "\n",
        "print(\"Loading dataset (this may take a minute)...\")\n",
        "data_set = load_dataset(DATASET_ID)   # loads all splits (here only 'train')\n",
        "print(\"Dataset is loaded successfully\")"
      ],
      "metadata": {
        "id": "GuM-dgZVU1Fa"
      },
      "id": "GuM-dgZVU1Fa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quick summary (splits, size)\n",
        "print(data_set)\n",
        "\n",
        "# Print column names and example\n",
        "split_name = list(data_set.keys())[0]      # should be 'train'\n",
        "print(\"Split:\", split_name)\n",
        "print(\"Columns:\", data_set[split_name].column_names)\n",
        "print(\"\\nOne sample (0):\")\n",
        "pprint(data_set[split_name][0])"
      ],
      "metadata": {
        "id": "JekcivKqlBdw"
      },
      "id": "JekcivKqlBdw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Preprocessing (Normalization & cleaning)"
      ],
      "metadata": {
        "id": "w_j5f_ZEYxKU"
      },
      "id": "w_j5f_ZEYxKU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1: Imports"
      ],
      "metadata": {
        "id": "eIuV5c4vaSdT"
      },
      "id": "eIuV5c4vaSdT"
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import re\n",
        "from hazm import Normalizer , word_tokenize , stopwords_list\n",
        "from datasets import DatasetDict\n",
        "from nltk.corpus import stopwords\n",
        "#import spacy"
      ],
      "metadata": {
        "id": "OB1Mo1EmZRwX"
      },
      "id": "OB1Mo1EmZRwX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "C4KEaVzqVy6V"
      },
      "id": "C4KEaVzqVy6V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2: Select a part of dataset"
      ],
      "metadata": {
        "id": "x97_OpzuDi8l"
      },
      "id": "x97_OpzuDi8l"
    },
    {
      "cell_type": "code",
      "source": [
        "# take a random sample of 500k\n",
        "ds = DatasetDict({\n",
        "    \"train\": data_set[\"train\"].shuffle(seed=42).select(range(500_000))\n",
        "})\n",
        "print(ds)"
      ],
      "metadata": {
        "id": "IwSiW_MADqFM"
      },
      "id": "IwSiW_MADqFM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3: Auto-detect which column is Persian / English & rename"
      ],
      "metadata": {
        "id": "PDkEaYqTaPJa"
      },
      "id": "PDkEaYqTaPJa"
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect Persian / English columns\n",
        "def is_persian(s):\n",
        "    if s is None:\n",
        "        return False\n",
        "    return bool(re.search(r'[\\u0600-\\u06FF]', s))\n",
        "\n",
        "cols = ds[split_name].column_names\n",
        "sample = ds[split_name][0]\n",
        "\n",
        "persian_col = None\n",
        "english_col = None\n",
        "for c in cols:\n",
        "    try:\n",
        "        if is_persian(sample[c]):\n",
        "            persian_col = c\n",
        "        else:\n",
        "            english_col = c\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "print(\"Detected Persian column:\", persian_col)\n",
        "print(\"Detected English column:\", english_col)\n",
        "\n",
        "\n",
        "# Helper functions to detect unwanted text\n",
        "def contains_english(text):\n",
        "    if text is None:\n",
        "        return False\n",
        "    return bool(re.search(r\"[A-Za-z]\", text))\n",
        "\n",
        "def contains_persian(text):\n",
        "    if text is None:\n",
        "        return False\n",
        "    return bool(re.search(r\"[\\u0600-\\u06FF]\", text))\n",
        "\n",
        "\n",
        "# Filter dataset\n",
        "filtered_dataset = ds[split_name].filter(\n",
        "    lambda x: not contains_english(x[persian_col]) and not contains_persian(x[english_col])\n",
        ")\n",
        "\n",
        "# Wrap back into DatasetDict to keep \"train\" key\n",
        "ds = {\"train\": filtered_dataset}\n",
        "print(ds)\n"
      ],
      "metadata": {
        "id": "PefOW2UDX_Ae"
      },
      "id": "PefOW2UDX_Ae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- There is a problem here. if we delete this way, all persian sentences with a single charachter of english will be deleted."
      ],
      "metadata": {
        "id": "NFOrvrh1YbKX"
      },
      "id": "NFOrvrh1YbKX"
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "id": "oT5nM2qUZZI_"
      },
      "id": "oT5nM2qUZZI_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4: Rename columns to standard names and run light cleaning"
      ],
      "metadata": {
        "id": "qhKse3AGaH7v"
      },
      "id": "qhKse3AGaH7v"
    },
    {
      "cell_type": "code",
      "source": [
        "def rename_and_select(example):\n",
        "    return {\"persian\": example[persian_col], \"english\": example[english_col]}\n",
        "\n",
        "print(\"Mapping and renaming columns...\")\n",
        "ds_simple = ds[split_name].map(rename_and_select, remove_columns=ds[split_name].column_names) # split_name=\"train\"\n",
        "\n",
        "print(\"Old columns:\", ds[split_name].column_names)\n",
        "print(\"New columns:\", ds_simple.column_names)"
      ],
      "metadata": {
        "id": "vI8KhTDEYLEe"
      },
      "id": "vI8KhTDEYLEe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds)\n",
        "print(ds_simple)"
      ],
      "metadata": {
        "id": "GCNvlxRqKKFZ"
      },
      "id": "GCNvlxRqKKFZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5: Cleaning helpers (Persian normalizer + English lite cleaning)"
      ],
      "metadata": {
        "id": "2JRJB26MaCyl"
      },
      "id": "2JRJB26MaCyl"
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = Normalizer()\n",
        "\n",
        "# Maps Arabic to Persian chars\n",
        "AR_TO_FA = {'\\u064A': '\\u06CC', '\\u0643': '\\u06A9'}\n",
        "# Zero-width and invisible characters\n",
        "ZERO_WIDTH = ['\\u200c', '\\u200f', '\\u202a', '\\u202b']\n",
        "\n",
        "PERSIAN_DIGITS = '۰۱۲۳۴۵۶۷۸۹'\n",
        "ASCII_DIGITS = '0123456789'\n",
        "\n",
        "# Persian + English stopwords\n",
        "persian_stopwords = set(stopwords_list())\n",
        "english_stopwords = set(stopwords.words(\"english\"))\n",
        "\n",
        "# --- Cleaning functions ---\n",
        "def replace_arabic_chars(text):\n",
        "    for a, f in AR_TO_FA.items():\n",
        "        text = text.replace(a, f)\n",
        "    return text\n",
        "\n",
        "def remove_zero_width(text):\n",
        "    for zw in ZERO_WIDTH:\n",
        "        text = text.replace(zw, \" \")\n",
        "    return text\n",
        "\n",
        "def normalize_digits(text):\n",
        "    for fa, en in zip(PERSIAN_DIGITS, ASCII_DIGITS):\n",
        "        text = text.replace(fa, en)\n",
        "    return text\n",
        "\n",
        "def clean_html(text):\n",
        "    return re.sub(r\"<.*?>\", \" \", text)\n",
        "\n",
        "def clean_urls(text):\n",
        "    return re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
        "\n",
        "def remove_unwanted_chars(text):\n",
        "    # Keep Persian, English letters, and basic spaces\n",
        "    return re.sub(r\"[^آ-یA-Za-z0-9\\s]\", \" \", text)\n",
        "\n",
        "def remove_extra_spaces(text):\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [t for t in tokens if t not in persian_stopwords and t.lower() not in english_stopwords]\n",
        "    return \" \".join(tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "bRTWAQKAY6Ve"
      },
      "id": "bRTWAQKAY6Ve",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_persian(text):\n",
        "    if text is None: return \"\"\n",
        "    text = str(text)\n",
        "    text = text.replace(\"٫\", \" \")\n",
        "    text = text.replace(\",\", \" \")\n",
        "    text = clean_html(text)\n",
        "    text = clean_urls(text)\n",
        "    text = replace_arabic_chars(text)\n",
        "    text = remove_zero_width(text)\n",
        "    text = normalize_digits(text)\n",
        "    text = normalizer.normalize(text)\n",
        "    text = normalize_digits(text)\n",
        "    text = remove_unwanted_chars(text)\n",
        "    text = remove_extra_spaces(text)\n",
        "    #text = remove_stopwords(text)\n",
        "    return text\n",
        "\n",
        "def clean_english(text):\n",
        "    if text is None: return \"\"\n",
        "    text = str(text).strip()\n",
        "    text = text.replace(\"٫\", \" \")\n",
        "    text = text.replace(\",\", \" \")\n",
        "    # optional: lowercasing (depends if you want to preserve casing)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = clean_html(text)\n",
        "    text = clean_urls(text)\n",
        "    text = remove_zero_width(text)\n",
        "    text = normalizer.normalize(text)\n",
        "    text = normalize_digits(text)\n",
        "    text = remove_unwanted_chars(text)\n",
        "    text = remove_extra_spaces(text)\n",
        "    #text = remove_stopwords(text)\n",
        "    return text\n",
        "\n",
        "# Quick test\n",
        "print(clean_persian(\"این٫  ۱۲۳ کتاب‌ هاست‎\"))\n",
        "print(clean_english(\" This IS  ,a Test! \"))\n",
        "print(clean_persian(\" کاتالوگ 4 5۵, ٫ مگابایت\"))\n",
        "print(clean_english(\" کاتالوگ ۵۴ 7, ٫ مگابایت\"))"
      ],
      "metadata": {
        "id": "i_g3l20cLy15"
      },
      "id": "i_g3l20cLy15",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.6: Apply cleaning"
      ],
      "metadata": {
        "id": "8mN-koSPZ8pD"
      },
      "id": "8mN-koSPZ8pD"
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply cleaning to dataset (use num_proc if Colab CPU allows parallelism)\n",
        "def clean_example(ex):\n",
        "    return {\n",
        "        \"persian\": clean_persian(ex[\"persian\"]),\n",
        "        \"english\": clean_english(ex[\"english\"])\n",
        "    }\n",
        "\n",
        "print(\"Cleaning dataset (batched)...\")\n",
        "ds_clean = ds_simple.map(clean_example, batched=False)  # batched=True can be faster but needs function change\n",
        "print(\"After cleaning sample:\", ds_clean[0])"
      ],
      "metadata": {
        "id": "xjGPDs0EZwfO"
      },
      "id": "xjGPDs0EZwfO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out empty or too-long sentences (token-length heuristics)\n",
        "MAX_CHARS = 512\n",
        "def filter_empty_or_long(ex):\n",
        "    if ex[\"persian\"].strip()==\"\" or ex[\"english\"].strip()==\"\":\n",
        "        return False\n",
        "    if len(ex[\"persian\"]) > MAX_CHARS or len(ex[\"english\"]) > MAX_CHARS:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "ds_clean = ds_clean.filter(filter_empty_or_long)\n",
        "print(\"Size after filter:\", len(ds_clean))"
      ],
      "metadata": {
        "id": "qYiE4AX5nHTP"
      },
      "id": "qYiE4AX5nHTP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"some examples:\")\n",
        "\n",
        "for i in range(5):\n",
        "  print(f\"\\n {i}-raw:\" ,(ds[split_name][i]))\n",
        "  print(f\"\\n {i}-Pe_En:\" ,(ds_simple[i]))\n",
        "  print(f\"\\n {i}-clean:\" ,(ds_clean[i]))\n",
        "  print(f\"\\n\")"
      ],
      "metadata": {
        "id": "tXozs4GdNcjc"
      },
      "id": "tXozs4GdNcjc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.6: Apply cleaning with removing stop-words"
      ],
      "metadata": {
        "id": "Nc-86RkPn3WD"
      },
      "id": "Nc-86RkPn3WD"
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_stopwords_persian(text):\n",
        "    if text is None: return \"\"\n",
        "    text = remove_stopwords(text)\n",
        "    return text\n",
        "\n",
        "def clean_stopwords_english(text):\n",
        "    if text is None: return \"\"\n",
        "    text = remove_stopwords(text)\n",
        "    return text\n",
        "\n",
        "# Quick test\n",
        "print(clean_stopwords_persian(clean_persian(\"این٫  ۱۲۳ کتاب‌ هاست‎\")))\n",
        "print(clean_stopwords_english(clean_english(\" This IS  ,a Test! \")))\n",
        "print(clean_stopwords_persian(clean_persian(\" کاتالوگ, ٫ مگابایت\")))\n",
        "print(clean_stopwords_english(clean_english(\" کاتالوگ, ٫ مگابایت\")))"
      ],
      "metadata": {
        "id": "-4fg8n50n-2C"
      },
      "id": "-4fg8n50n-2C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply stop-words cleaning to dataset (use num_proc if Colab CPU allows parallelism)\n",
        "def clean_example(ex):\n",
        "    return {\n",
        "        \"persian\": clean_stopwords_persian(ex[\"persian\"]),\n",
        "        \"english\": clean_stopwords_english(ex[\"english\"])\n",
        "    }\n",
        "\n",
        "print(\"Cleaning dataset ...\")\n",
        "ds_stopwords_clean = ds_clean.map(clean_example, batched=False)  # batched=True can be faster but needs function change\n",
        "print(\"After stop-words cleaning sample:\", ds_stopwords_clean[0])"
      ],
      "metadata": {
        "id": "1bOJHWo6oOdB"
      },
      "id": "1bOJHWo6oOdB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"some examples:\")\n",
        "\n",
        "for i in range(5):\n",
        "  print(f\"\\n {i}-raw:\" ,(ds[split_name][i]))\n",
        "  #print(f\"\\n {i}-Pe_En:\" ,(ds_simple[i]))\n",
        "  print(f\"\\n {i}-clean:\" ,(ds_clean[i]))\n",
        "  print(f\"\\n {i}-clean_stopwords:\" ,(ds_stopwords_clean[i]))\n",
        "  print(f\"\\n\")"
      ],
      "metadata": {
        "id": "WaJD1e-eoyas"
      },
      "id": "WaJD1e-eoyas",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.8: Create train/validation/test splits"
      ],
      "metadata": {
        "id": "tQe3XlzNazuD"
      },
      "id": "tQe3XlzNazuD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.8.1: ds_clean**"
      ],
      "metadata": {
        "id": "2NLB29IDrTH2"
      },
      "id": "2NLB29IDrTH2"
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(ds_clean, seed: int = 42):\n",
        "\n",
        "    total = len(ds_clean)\n",
        "    print(\"Total pairs:\", total)\n",
        "\n",
        "    if total > 1000:\n",
        "        # Large dataset: ~3% holdout, split into val/test (1.5% each)\n",
        "        split1 = ds_clean.train_test_split(test_size=0.03, seed=seed, shuffle=True)\n",
        "        hold = split1['test'].train_test_split(test_size=0.5, seed=seed)\n",
        "        dataset_splits = DatasetDict({\n",
        "            'train': split1['train'],\n",
        "            'validation': hold['train'],\n",
        "            'test': hold['test']\n",
        "        })\n",
        "    else:\n",
        "        # Small dataset: 80/10/10\n",
        "        split1 = ds_clean.train_test_split(test_size=0.2, seed=seed, shuffle=True)\n",
        "        hold = split1['test'].train_test_split(test_size=0.5, seed=seed)\n",
        "        dataset_splits = DatasetDict({\n",
        "            'train': split1['train'],\n",
        "            'validation': hold['train'],\n",
        "            'test': hold['test']\n",
        "        })\n",
        "\n",
        "    # Print sizes\n",
        "    for k in dataset_splits:\n",
        "        print(f\"{k}: {len(dataset_splits[k])}\")\n",
        "\n",
        "    # Peek at one example\n",
        "    print(\"\\nExample pair (train[10]):\")\n",
        "    print(dataset_splits['train'][10])\n",
        "\n",
        "    return dataset_splits\n"
      ],
      "metadata": {
        "id": "0t8j2Kxhwgyb"
      },
      "id": "0t8j2Kxhwgyb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_splits = split_dataset(ds_clean)"
      ],
      "metadata": {
        "id": "GwdmQ9Nea4Y2"
      },
      "id": "GwdmQ9Nea4Y2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you want to implement \"Stop-words Removing\"\n",
        "#dataset_splits = split_dataset(ds_stopwords_clean)"
      ],
      "metadata": {
        "id": "Qbzbuy3xxFHF"
      },
      "id": "Qbzbuy3xxFHF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.8: Save to disk / optional Google Drive mount"
      ],
      "metadata": {
        "id": "nIRHmhm7bS6U"
      },
      "id": "nIRHmhm7bS6U"
    },
    {
      "cell_type": "code",
      "source": [
        "# Option A: save to Colab local storage\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "dataset_splits['train'].to_csv(\"data/train.csv\")\n",
        "dataset_splits['validation'].to_csv(\"data/validation.csv\")\n",
        "dataset_splits['test'].to_csv(\"data/test.csv\")\n",
        "print(\"Saved CSVs to ./data/\")"
      ],
      "metadata": {
        "id": "Udn4bc_Xa4A5"
      },
      "id": "Udn4bc_Xa4A5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option B: save to Google Drive (uncomment to use)\n",
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "out_dir = \"/content/drive/MyDrive/persian_translation_dataset\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "dataset_splits['train'].to_csv(os.path.join(out_dir,\"train.csv\"))\n",
        "dataset_splits['validation'].to_csv(os.path.join(out_dir,\"validation.csv\"))\n",
        "dataset_splits['test'].to_csv(os.path.join(out_dir,\"test.csv\"))\n",
        "print(\"Saved CSVs to Google Drive:\", out_dir)\n",
        "'''"
      ],
      "metadata": {
        "id": "Mc9Kk6LU9F5V"
      },
      "id": "Mc9Kk6LU9F5V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cdf03190",
      "metadata": {
        "id": "cdf03190"
      },
      "source": [
        "## 5. POS tagging (Hazm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e76f950a",
      "metadata": {
        "id": "e76f950a"
      },
      "source": [
        "## 6. Named Entity Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4a8ab1b",
      "metadata": {
        "id": "b4a8ab1b"
      },
      "source": [
        "## 7. Tokenization & embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7a99d07",
      "metadata": {
        "id": "c7a99d07"
      },
      "source": [
        "## 8. Seq2Seq model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9640856",
      "metadata": {
        "id": "a9640856"
      },
      "source": [
        "## 9. Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbb8ed52",
      "metadata": {
        "id": "dbb8ed52"
      },
      "source": [
        "## End of Notebook\n",
        "This Colab notebook covers preprocessing, POS tagging, NER, tokenization, Seq2Seq model, and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aAtpXXIMxZNQ"
      },
      "id": "aAtpXXIMxZNQ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}