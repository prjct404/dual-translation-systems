{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eb6719d6",
      "metadata": {
        "id": "eb6719d6"
      },
      "source": [
        "# Persian-English Translator Project (Classical Seq2Seq)\n",
        "\n",
        "This notebook implements a classical Persian↔English translator using Seq2Seq (LSTM). It includes preprocessing, POS tagging, NER, tokenization, embedding, model design, training, and evaluation.\n",
        "\n",
        "Before training embeddings and classifiers, the raw shenasa/English-Persian-Parallel-Dataset was cleaned, normalized, and tokenized. This step is essential for Persian (Farsi) text, which often mixes scripts, punctuation styles, and contains noise.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38a0a6f3",
      "metadata": {
        "id": "38a0a6f3"
      },
      "source": [
        "## Table of contents\n",
        "1. Project goals\n",
        "2. Environment setup\n",
        "3. Data collection\n",
        "4. Preprocessing\n",
        "5. POS tagging\n",
        "6. NER\n",
        "7. Tokenization & Embedding\n",
        "8. Seq2Seq model\n",
        "9. Evaluation\n",
        "10. Analysis and report\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2d98dff",
      "metadata": {
        "id": "d2d98dff"
      },
      "source": [
        "## 2. Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GhPvpqmRpT5F",
      "metadata": {
        "collapsed": true,
        "id": "GhPvpqmRpT5F"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install numpy pandas matplotlib scikit-learn tensorflow keras hazm parsivar sacrebleu nltk sentencepiece\n",
        "# Optional: for advanced Persian NER datasets and models\n",
        "!pip install datasets transformers seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfGqwImhJAVY",
      "metadata": {
        "id": "dfGqwImhJAVY"
      },
      "outputs": [],
      "source": [
        "#%load_ext cudf.pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XByeLxawl6EB",
      "metadata": {
        "id": "XByeLxawl6EB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "import hazm\n",
        "import parsivar\n",
        "import sacrebleu\n",
        "import nltk\n",
        "import sentencepiece\n",
        "import datasets\n",
        "import transformers\n",
        "import seqeval"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79TV9l_bU0d0",
      "metadata": {
        "id": "79TV9l_bU0d0"
      },
      "source": [
        "## 3. Data collection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fQQSIYqMVX_I",
      "metadata": {
        "id": "fQQSIYqMVX_I"
      },
      "source": [
        "### 3.1: Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UQCFSNWXYRgQ",
      "metadata": {
        "id": "UQCFSNWXYRgQ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "import re, os\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WbmhaWaelC5T",
      "metadata": {
        "id": "WbmhaWaelC5T"
      },
      "source": [
        "### 3.2: Load dataset and inspect (normal load)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GuM-dgZVU1Fa",
      "metadata": {
        "id": "GuM-dgZVU1Fa"
      },
      "outputs": [],
      "source": [
        "# Data collection: load the Hugging Face dataset (full, non-streaming)\n",
        "\n",
        "DATASET_ID = \"shenasa/English-Persian-Parallel-Dataset\"\n",
        "\n",
        "print(\"Loading dataset (this may take a minute)...\")\n",
        "data_set = load_dataset(DATASET_ID)   # loads all splits (here only 'train')\n",
        "print(\"Dataset is loaded successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JekcivKqlBdw",
      "metadata": {
        "id": "JekcivKqlBdw"
      },
      "outputs": [],
      "source": [
        "# quick summary (splits, size)\n",
        "print(data_set)\n",
        "\n",
        "# Print column names and example\n",
        "split_name = list(data_set.keys())[0]      # should be 'train'\n",
        "print(\"Split:\", split_name)\n",
        "print(\"Columns:\", data_set[split_name].column_names)\n",
        "print(\"\\nOne sample (0):\")\n",
        "pprint(data_set[split_name][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "w_j5f_ZEYxKU",
      "metadata": {
        "id": "w_j5f_ZEYxKU"
      },
      "source": [
        "## 4. Preprocessing (Normalization & cleaning)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eIuV5c4vaSdT",
      "metadata": {
        "id": "eIuV5c4vaSdT"
      },
      "source": [
        "### 4.1: Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OB1Mo1EmZRwX",
      "metadata": {
        "id": "OB1Mo1EmZRwX"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "import re\n",
        "from hazm import Normalizer , word_tokenize , stopwords_list\n",
        "from datasets import DatasetDict\n",
        "from nltk.corpus import stopwords\n",
        "#import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C4KEaVzqVy6V",
      "metadata": {
        "id": "C4KEaVzqVy6V"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x97_OpzuDi8l",
      "metadata": {
        "id": "x97_OpzuDi8l"
      },
      "source": [
        "### 4.2: Select a part of dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IwSiW_MADqFM",
      "metadata": {
        "id": "IwSiW_MADqFM"
      },
      "outputs": [],
      "source": [
        "# take a random sample of 500k\n",
        "ds = DatasetDict({\n",
        "    \"train\": data_set[\"train\"].shuffle(seed=42).select(range(500_000))\n",
        "})\n",
        "print(ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PDkEaYqTaPJa",
      "metadata": {
        "id": "PDkEaYqTaPJa"
      },
      "source": [
        "### 4.3: Auto-detect which column is Persian / English & rename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PefOW2UDX_Ae",
      "metadata": {
        "id": "PefOW2UDX_Ae"
      },
      "outputs": [],
      "source": [
        "# Detect Persian / English columns\n",
        "def is_persian(s):\n",
        "    if s is None:\n",
        "        return False\n",
        "    return bool(re.search(r'[\\u0600-\\u06FF]', s))\n",
        "\n",
        "cols = ds[split_name].column_names\n",
        "sample = ds[split_name][0]\n",
        "\n",
        "persian_col = None\n",
        "english_col = None\n",
        "for c in cols:\n",
        "    try:\n",
        "        if is_persian(sample[c]):\n",
        "            persian_col = c\n",
        "        else:\n",
        "            english_col = c\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "print(\"Detected Persian column:\", persian_col)\n",
        "print(\"Detected English column:\", english_col)\n",
        "\n",
        "\n",
        "# Helper functions to detect unwanted text\n",
        "def contains_english(text):\n",
        "    if text is None:\n",
        "        return False\n",
        "    return bool(re.search(r\"[A-Za-z]\", text))\n",
        "\n",
        "def contains_persian(text):\n",
        "    if text is None:\n",
        "        return False\n",
        "    return bool(re.search(r\"[\\u0600-\\u06FF]\", text))\n",
        "\n",
        "\n",
        "# Filter dataset\n",
        "filtered_dataset = ds[split_name].filter(\n",
        "    lambda x: not contains_english(x[persian_col]) and not contains_persian(x[english_col])\n",
        ")\n",
        "\n",
        "# Wrap back into DatasetDict to keep \"train\" key\n",
        "ds = {\"train\": filtered_dataset}\n",
        "print(ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NFOrvrh1YbKX",
      "metadata": {
        "id": "NFOrvrh1YbKX"
      },
      "source": [
        "- There is a problem here. if we delete this way, all persian sentences with a single charachter of english will be deleted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oT5nM2qUZZI_",
      "metadata": {
        "id": "oT5nM2qUZZI_"
      },
      "outputs": [],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qhKse3AGaH7v",
      "metadata": {
        "id": "qhKse3AGaH7v"
      },
      "source": [
        "### 4.4: Rename columns to standard names and run light cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vI8KhTDEYLEe",
      "metadata": {
        "id": "vI8KhTDEYLEe"
      },
      "outputs": [],
      "source": [
        "def rename_and_select(example):\n",
        "    return {\"persian\": example[persian_col], \"english\": example[english_col]}\n",
        "\n",
        "print(\"Mapping and renaming columns...\")\n",
        "ds_simple = ds[split_name].map(rename_and_select, remove_columns=ds[split_name].column_names) # split_name=\"train\"\n",
        "\n",
        "print(\"Old columns:\", ds[split_name].column_names)\n",
        "print(\"New columns:\", ds_simple.column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GCNvlxRqKKFZ",
      "metadata": {
        "id": "GCNvlxRqKKFZ"
      },
      "outputs": [],
      "source": [
        "print(ds)\n",
        "print(ds_simple)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2JRJB26MaCyl",
      "metadata": {
        "id": "2JRJB26MaCyl"
      },
      "source": [
        "### 4.5: Cleaning helpers (Persian normalizer + English lite cleaning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bRTWAQKAY6Ve",
      "metadata": {
        "id": "bRTWAQKAY6Ve"
      },
      "outputs": [],
      "source": [
        "normalizer = Normalizer()\n",
        "\n",
        "# Maps Arabic to Persian chars\n",
        "AR_TO_FA = {'\\u064A': '\\u06CC', '\\u0643': '\\u06A9'}\n",
        "# Zero-width and invisible characters\n",
        "ZERO_WIDTH = ['\\u200c', '\\u200f', '\\u202a', '\\u202b']\n",
        "\n",
        "PERSIAN_DIGITS = '۰۱۲۳۴۵۶۷۸۹'\n",
        "ASCII_DIGITS = '0123456789'\n",
        "\n",
        "# Persian + English stopwords\n",
        "persian_stopwords = set(stopwords_list())\n",
        "english_stopwords = set(stopwords.words(\"english\"))\n",
        "\n",
        "# --- Cleaning functions ---\n",
        "def replace_arabic_chars(text):\n",
        "    for a, f in AR_TO_FA.items():\n",
        "        text = text.replace(a, f)\n",
        "    return text\n",
        "\n",
        "def remove_zero_width(text):\n",
        "    for zw in ZERO_WIDTH:\n",
        "        text = text.replace(zw, \" \")\n",
        "    return text\n",
        "\n",
        "def normalize_digits(text):\n",
        "    for fa, en in zip(PERSIAN_DIGITS, ASCII_DIGITS):\n",
        "        text = text.replace(fa, en)\n",
        "    return text\n",
        "\n",
        "def clean_html(text):\n",
        "    return re.sub(r\"<.*?>\", \" \", text)\n",
        "\n",
        "def clean_urls(text):\n",
        "    return re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
        "\n",
        "def remove_unwanted_chars(text):\n",
        "    # Keep Persian, English letters, and basic spaces\n",
        "    return re.sub(r\"[^آ-یA-Za-z0-9\\s]\", \" \", text)\n",
        "\n",
        "def remove_extra_spaces(text):\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [t for t in tokens if t not in persian_stopwords and t.lower() not in english_stopwords]\n",
        "    return \" \".join(tokens)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i_g3l20cLy15",
      "metadata": {
        "id": "i_g3l20cLy15"
      },
      "outputs": [],
      "source": [
        "def clean_persian(text):\n",
        "    if text is None: return \"\"\n",
        "    text = str(text)\n",
        "    text = text.replace(\"٫\", \" \")\n",
        "    text = text.replace(\",\", \" \")\n",
        "    text = clean_html(text)\n",
        "    text = clean_urls(text)\n",
        "    text = replace_arabic_chars(text)\n",
        "    text = remove_zero_width(text)\n",
        "    text = normalize_digits(text)\n",
        "    text = normalizer.normalize(text)\n",
        "    text = normalize_digits(text)\n",
        "    text = remove_unwanted_chars(text)\n",
        "    text = remove_extra_spaces(text)\n",
        "    #text = remove_stopwords(text)\n",
        "    return text\n",
        "\n",
        "def clean_english(text):\n",
        "    if text is None: return \"\"\n",
        "    text = str(text).strip()\n",
        "    text = text.replace(\"٫\", \" \")\n",
        "    text = text.replace(\",\", \" \")\n",
        "    # optional: lowercasing (depends if you want to preserve casing)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = clean_html(text)\n",
        "    text = clean_urls(text)\n",
        "    text = remove_zero_width(text)\n",
        "    text = normalizer.normalize(text)\n",
        "    text = normalize_digits(text)\n",
        "    text = remove_unwanted_chars(text)\n",
        "    text = remove_extra_spaces(text)\n",
        "    #text = remove_stopwords(text)\n",
        "    return text\n",
        "\n",
        "# Quick test\n",
        "print(clean_persian(\"این٫  ۱۲۳ کتاب‌ هاست‎\"))\n",
        "print(clean_english(\" This IS  ,a Test! \"))\n",
        "print(clean_persian(\" کاتالوگ 4 5۵, ٫ مگابایت\"))\n",
        "print(clean_english(\" کاتالوگ ۵۴ 7, ٫ مگابایت\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8mN-koSPZ8pD",
      "metadata": {
        "id": "8mN-koSPZ8pD"
      },
      "source": [
        "### 4.6: Apply cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xjGPDs0EZwfO",
      "metadata": {
        "id": "xjGPDs0EZwfO"
      },
      "outputs": [],
      "source": [
        "# Apply cleaning to dataset (use num_proc if Colab CPU allows parallelism)\n",
        "def clean_example(ex):\n",
        "    return {\n",
        "        \"persian\": clean_persian(ex[\"persian\"]),\n",
        "        \"english\": clean_english(ex[\"english\"])\n",
        "    }\n",
        "\n",
        "print(\"Cleaning dataset (batched)...\")\n",
        "ds_clean = ds_simple.map(clean_example, batched=False)  # batched=True can be faster but needs function change\n",
        "print(\"After cleaning sample:\", ds_clean[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qYiE4AX5nHTP",
      "metadata": {
        "id": "qYiE4AX5nHTP"
      },
      "outputs": [],
      "source": [
        "# Filter out empty or too-long sentences (token-length heuristics)\n",
        "MAX_CHARS = 512\n",
        "def filter_empty_or_long(ex):\n",
        "    if ex[\"persian\"].strip()==\"\" or ex[\"english\"].strip()==\"\":\n",
        "        return False\n",
        "    if len(ex[\"persian\"]) > MAX_CHARS or len(ex[\"english\"]) > MAX_CHARS:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "ds_clean = ds_clean.filter(filter_empty_or_long)\n",
        "print(\"Size after filter:\", len(ds_clean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tXozs4GdNcjc",
      "metadata": {
        "id": "tXozs4GdNcjc"
      },
      "outputs": [],
      "source": [
        "print(\"some examples:\")\n",
        "\n",
        "for i in range(5):\n",
        "  print(f\"\\n {i}-raw:\" ,(ds[split_name][i]))\n",
        "  print(f\"\\n {i}-Pe_En:\" ,(ds_simple[i]))\n",
        "  print(f\"\\n {i}-clean:\" ,(ds_clean[i]))\n",
        "  print(f\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Nc-86RkPn3WD",
      "metadata": {
        "id": "Nc-86RkPn3WD"
      },
      "source": [
        "### 4.6: Apply cleaning with removing stop-words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-4fg8n50n-2C",
      "metadata": {
        "id": "-4fg8n50n-2C"
      },
      "outputs": [],
      "source": [
        "def clean_stopwords_persian(text):\n",
        "    if text is None: return \"\"\n",
        "    text = remove_stopwords(text)\n",
        "    return text\n",
        "\n",
        "def clean_stopwords_english(text):\n",
        "    if text is None: return \"\"\n",
        "    text = remove_stopwords(text)\n",
        "    return text\n",
        "\n",
        "# Quick test\n",
        "print(clean_stopwords_persian(clean_persian(\"این٫  ۱۲۳ کتاب‌ هاست‎\")))\n",
        "print(clean_stopwords_english(clean_english(\" This IS  ,a Test! \")))\n",
        "print(clean_stopwords_persian(clean_persian(\" کاتالوگ, ٫ مگابایت\")))\n",
        "print(clean_stopwords_english(clean_english(\" کاتالوگ, ٫ مگابایت\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bOJHWo6oOdB",
      "metadata": {
        "id": "1bOJHWo6oOdB"
      },
      "outputs": [],
      "source": [
        "# Apply stop-words cleaning to dataset (use num_proc if Colab CPU allows parallelism)\n",
        "def clean_example(ex):\n",
        "    return {\n",
        "        \"persian\": clean_stopwords_persian(ex[\"persian\"]),\n",
        "        \"english\": clean_stopwords_english(ex[\"english\"])\n",
        "    }\n",
        "\n",
        "print(\"Cleaning dataset ...\")\n",
        "ds_stopwords_clean = ds_clean.map(clean_example, batched=False)  # batched=True can be faster but needs function change\n",
        "print(\"After stop-words cleaning sample:\", ds_stopwords_clean[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WaJD1e-eoyas",
      "metadata": {
        "id": "WaJD1e-eoyas"
      },
      "outputs": [],
      "source": [
        "print(\"some examples:\")\n",
        "\n",
        "for i in range(5):\n",
        "  print(f\"\\n {i}-raw:\" ,(ds[split_name][i]))\n",
        "  #print(f\"\\n {i}-Pe_En:\" ,(ds_simple[i]))\n",
        "  print(f\"\\n {i}-clean:\" ,(ds_clean[i]))\n",
        "  print(f\"\\n {i}-clean_stopwords:\" ,(ds_stopwords_clean[i]))\n",
        "  print(f\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tQe3XlzNazuD",
      "metadata": {
        "id": "tQe3XlzNazuD"
      },
      "source": [
        "### 4.8: Create train/validation/test splits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2NLB29IDrTH2",
      "metadata": {
        "id": "2NLB29IDrTH2"
      },
      "source": [
        "**4.8.1: ds_clean**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0t8j2Kxhwgyb",
      "metadata": {
        "id": "0t8j2Kxhwgyb"
      },
      "outputs": [],
      "source": [
        "def split_dataset(ds_clean, seed: int = 42):\n",
        "\n",
        "    total = len(ds_clean)\n",
        "    print(\"Total pairs:\", total)\n",
        "\n",
        "    if total > 1000:\n",
        "        # Large dataset: ~3% holdout, split into val/test (1.5% each)\n",
        "        split1 = ds_clean.train_test_split(test_size=0.03, seed=seed, shuffle=True)\n",
        "        hold = split1['test'].train_test_split(test_size=0.5, seed=seed)\n",
        "        dataset_splits = DatasetDict({\n",
        "            'train': split1['train'],\n",
        "            'validation': hold['train'],\n",
        "            'test': hold['test']\n",
        "        })\n",
        "    else:\n",
        "        # Small dataset: 80/10/10\n",
        "        split1 = ds_clean.train_test_split(test_size=0.2, seed=seed, shuffle=True)\n",
        "        hold = split1['test'].train_test_split(test_size=0.5, seed=seed)\n",
        "        dataset_splits = DatasetDict({\n",
        "            'train': split1['train'],\n",
        "            'validation': hold['train'],\n",
        "            'test': hold['test']\n",
        "        })\n",
        "\n",
        "    # Print sizes\n",
        "    for k in dataset_splits:\n",
        "        print(f\"{k}: {len(dataset_splits[k])}\")\n",
        "\n",
        "    # Peek at one example\n",
        "    print(\"\\nExample pair (train[10]):\")\n",
        "    print(dataset_splits['train'][10])\n",
        "\n",
        "    return dataset_splits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GwdmQ9Nea4Y2",
      "metadata": {
        "id": "GwdmQ9Nea4Y2"
      },
      "outputs": [],
      "source": [
        "dataset_splits = split_dataset(ds_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qbzbuy3xxFHF",
      "metadata": {
        "id": "Qbzbuy3xxFHF"
      },
      "outputs": [],
      "source": [
        "# If you want to implement \"Stop-words Removing\"\n",
        "#dataset_splits = split_dataset(ds_stopwords_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nIRHmhm7bS6U",
      "metadata": {
        "id": "nIRHmhm7bS6U"
      },
      "source": [
        "### 4.8: Save to disk / optional Google Drive mount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Udn4bc_Xa4A5",
      "metadata": {
        "id": "Udn4bc_Xa4A5"
      },
      "outputs": [],
      "source": [
        "# Option A: save to Colab local storage\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "dataset_splits['train'].to_csv(\"data/train.csv\")\n",
        "dataset_splits['validation'].to_csv(\"data/validation.csv\")\n",
        "dataset_splits['test'].to_csv(\"data/test.csv\")\n",
        "print(\"Saved CSVs to ./data/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Mc9Kk6LU9F5V",
      "metadata": {
        "id": "Mc9Kk6LU9F5V"
      },
      "outputs": [],
      "source": [
        "# Option B: save to Google Drive (uncomment to use)\n",
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "out_dir = \"/content/drive/MyDrive/persian_translation_dataset\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "dataset_splits['train'].to_csv(os.path.join(out_dir,\"train.csv\"))\n",
        "dataset_splits['validation'].to_csv(os.path.join(out_dir,\"validation.csv\"))\n",
        "dataset_splits['test'].to_csv(os.path.join(out_dir,\"test.csv\"))\n",
        "print(\"Saved CSVs to Google Drive:\", out_dir)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdf03190",
      "metadata": {
        "id": "cdf03190"
      },
      "source": [
        "## 5. POS tagging (Hazm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa7f44ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "#test hazm pos_tagger model\n",
        "tagger = POSTagger(model='pos_tagger.model')\n",
        "tagger.tag(word_tokenize('ما بسیار کتاب می‌خوانیم'))\n",
        "[('ما', 'PRO'), ('بسیار', 'ADV'), ('کتاب', 'N'), ('می‌خوانیم', 'V')]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f48f7e80",
      "metadata": {},
      "outputs": [],
      "source": [
        "from hazm import Normalizer, word_tokenize, POSTagger\n",
        "\n",
        "# we should run this code for each train/test/validation datasets\n",
        "\n",
        "# ==== config (edit as needed) ====\n",
        "IN_CSV   = \"data/train.csv\"            # must contain a 'persian' column (and optionally 'english')\n",
        "OUT_CSV  = \"data/train_pos.csv\"\n",
        "MODEL    = \"pos_tagger.model\"         # e.g. \"resources/postagger.model\"\n",
        "DELIM    = \",\"                        # change if your CSV uses another delimiter\n",
        "NROWS    = None                       # set to an int (e.g., 10) to sample only first N rows\n",
        "# =================================\n",
        "\n",
        "# load data\n",
        "df = pd.read_csv(IN_CSV, delimiter=DELIM, nrows=NROWS)\n",
        "\n",
        "# hazm components\n",
        "tagger = POSTagger(model=MODEL)\n",
        "norm = Normalizer(persian_numbers=True)\n",
        "\n",
        "def pos_tags_only(text: str) -> list[str]:\n",
        "    \"\"\"Return POS tag sequence (no tokens), one tag per token.\"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return []\n",
        "    # normalize + tokenize with the SAME toolchain you'll use elsewhere\n",
        "    toks = word_tokenize(norm.normalize(text))\n",
        "    tagged = tagger.tag(toks)              # [(tok, POS), ...]\n",
        "    return [pos for _, pos in tagged]      # keep only POS\n",
        "\n",
        "def tokenize(text: str) -> list[str]:\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return []\n",
        "    return word_tokenize(norm.normalize(text))\n",
        "\n",
        "# build columns\n",
        "df[\"persian_tok\"] = df[\"persian\"].apply(tokenize)\n",
        "df[\"persian_pos\"] = df[\"persian\"].apply(lambda s: \" \".join(pos_tags_only(s)))\n",
        "\n",
        "# (optional) quick alignment check\n",
        "def count_ws(s): return len(str(s).split())\n",
        "align_ok = []\n",
        "bad_rows = []\n",
        "for i, (sent, tags_str) in enumerate(zip(df[\"persian\"], df[\"persian_pos\"])):\n",
        "    n_tok = len(tokenize(sent))\n",
        "    n_tag = count_ws(tags_str)\n",
        "    ok = (n_tok == n_tag)\n",
        "    align_ok.append(ok)\n",
        "    if not ok and len(bad_rows) < 10:\n",
        "        bad_rows.append((i, n_tok, n_tag, sent, tags_str))\n",
        "\n",
        "df[\"pos_align_ok\"] = align_ok\n",
        "\n",
        "# save\n",
        "cols_to_save = [c for c in [\"persian\", \"english\", \"persian_pos\"] if c in df.columns]\n",
        "df[cols_to_save].to_csv(OUT_CSV, index=False)\n",
        "print(f\"Saved -> {OUT_CSV}\")\n",
        "print(f\"Alignment OK: {sum(align_ok)}/{len(align_ok)} \"\n",
        "      f\"({100*sum(align_ok)/max(1,len(align_ok)):.1f}%)\")\n",
        "\n",
        "if bad_rows:\n",
        "    print(\"\\nFirst few misaligned rows (index, n_tok, n_tag):\")\n",
        "    for i, n_tok, n_tag, sent, tags in bad_rows:\n",
        "        print(f\"- row {i}: tokens={n_tok}, tags={n_tag}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f1bbb69",
      "metadata": {},
      "source": [
        "now we have data with POS tagging\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e76f950a",
      "metadata": {
        "id": "e76f950a"
      },
      "source": [
        "## 6. Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "399c68eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "ner_fa = pipeline(\n",
        "    \"token-classification\",\n",
        "    model=\"HooshvareLab/bert-fa-base-uncased-ner-peyma\",\n",
        "    tokenizer=\"HooshvareLab/bert-fa-base-uncased-ner-peyma\",\n",
        "    aggregation_strategy=\"simple\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eea6a223",
      "metadata": {},
      "source": [
        "\n",
        "run this for each Train/Test/Validation dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e68cf1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import json\n",
        "\n",
        "# --- Load CSV (expects a column named \"persian\") ---\n",
        "ds = load_dataset(\"csv\", data_files=\"data/train_pos.csv\", split=\"train\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "226611e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import re\n",
        "import json\n",
        "\n",
        "\n",
        "# --- NER batch function: emits per-token tags like \"0 0 0 LOC 0\" ---\n",
        "def run_ner(batch):\n",
        "    texts = batch[\"persian\"]                  # list[str]\n",
        "    ents_list = ner_fa(texts)                 # list[list[dict]] ; aggregation_strategy=\"simple\"\n",
        "\n",
        "    def norm_label(lbl: str) -> str:\n",
        "        if not lbl:\n",
        "            return \"0\"\n",
        "        s = str(lbl).strip()\n",
        "        if s[:2] in (\"B_\", \"I_\", \"B-\", \"I-\"):\n",
        "            s = s[2:]\n",
        "        s = s.upper()\n",
        "        alias = {\n",
        "            \"PER\": \"PERSON\", \"PERS\": \"PERSON\", \"PERSON\": \"PERSON\",\n",
        "            \"ORG\": \"ORG\", \"ORGANIZATION\": \"ORG\",\n",
        "            \"LOC\": \"LOCATION\", \"LOCATION\": \"LOCATION\", \"GPE\": \"LOCATION\",\n",
        "            \"FAC\": \"FACILITY\", \"FACILITY\": \"FACILITY\",\n",
        "            \"DAT\": \"DATE\", \"DATE\": \"DATE\",\n",
        "            \"TIM\": \"TIME\", \"TIME\": \"TIME\",\n",
        "            \"MON\": \"MONEY\", \"MONEY\": \"MONEY\",\n",
        "            \"PCT\": \"PERCENT\", \"PERCENT\": \"PERCENT\",\n",
        "            \"QUANTITY\": \"QUANTITY\", \"CARDINAL\": \"CARDINAL\", \"ORDINAL\": \"ORDINAL\",\n",
        "            \"MISC\": \"MISC\", \"EVENT\": \"EVENT\", \"PRODUCT\": \"PRODUCT\",\n",
        "            \"WORK_OF_ART\": \"WORK_OF_ART\", \"LAW\": \"LAW\", \"LANGUAGE\": \"LANGUAGE\",\n",
        "            \"NORP\": \"NORP\",\n",
        "        }\n",
        "        return alias.get(s, s)\n",
        "\n",
        "    def tag_sequence(text: str, ents: list[dict]) -> str:\n",
        "        \"\"\"\n",
        "        Tokenize by whitespace (runs of non-space chars) and align labels by char spans.\n",
        "        Non-entity tokens get \"0\". If a token overlaps any entity span, it gets that entity's label.\n",
        "        \"\"\"\n",
        "        # 1) whitespace tokens with char spans\n",
        "        tokens = [(m.group(0), m.start(), m.end()) for m in re.finditer(r\"\\S+\", text or \"\")]\n",
        "        # 2) entity spans (start,end,label)\n",
        "        spans = []\n",
        "        for e in ents or []:\n",
        "            lbl = e.get(\"entity_group\") or e.get(\"entity\") or \"\"\n",
        "            start = e.get(\"start\")\n",
        "            end = e.get(\"end\")\n",
        "            if lbl and start is not None and end is not None:\n",
        "                spans.append((int(start), int(end), norm_label(lbl)))\n",
        "        # 3) assign label per token (simple overlap rule)\n",
        "        tags = []\n",
        "        for _, t0, t1 in tokens:\n",
        "            tag = \"0\"\n",
        "            for s0, s1, lab in spans:\n",
        "                if t0 < s1 and t1 > s0:  # overlap\n",
        "                    tag = lab\n",
        "                    break\n",
        "            tags.append(tag)\n",
        "        return \" \".join(tags)\n",
        "\n",
        "    # Build outputs\n",
        "    seqs = []\n",
        "    for text, ents in zip(texts, ents_list):\n",
        "        seqs.append(tag_sequence(text, ents))\n",
        "\n",
        "    batch[\"fa_ner_seq\"] = seqs   # e.g., \"0 0 0 LOC 0\"\n",
        "    return batch\n",
        "\n",
        "\n",
        "# ---- Map over dataset ----\n",
        "# NOTE: assumes you already have `ds` and `ner_fa`.\n",
        "ds_out = ds.map(run_ner, batched=True, batch_size=32)\n",
        "\n",
        "# ---- Save with fa_ner_seq as the LAST column ----\n",
        "# We’ll keep original column order and append the new column at the end.\n",
        "orig_cols = ds.column_names\n",
        "wanted_order = orig_cols + [\"fa_ner_seq\"]\n",
        "wanted_order = [c for c in wanted_order if c in ds_out.column_names]  # robust if columns vary\n",
        "\n",
        "# Option A (simple & explicit control via pandas)\n",
        "df = ds_out.to_pandas()\n",
        "df = df[wanted_order]\n",
        "df.to_csv(\"data/train_pos_ner.csv\", index=False)\n",
        "print(\"Saved train_pos_ner.csv with 'fa_ner_seq' appended as the last column.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a3fcddd",
      "metadata": {},
      "source": [
        "At the end we have full dataset with preprocessing / POS /NER\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4a8ab1b",
      "metadata": {
        "id": "b4a8ab1b"
      },
      "source": [
        "## 7. Tokenization & embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7a99d07",
      "metadata": {
        "id": "c7a99d07"
      },
      "source": [
        "## 8. Seq2Seq model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9640856",
      "metadata": {
        "id": "a9640856"
      },
      "source": [
        "## 9. Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbb8ed52",
      "metadata": {
        "id": "dbb8ed52"
      },
      "source": [
        "## End of Notebook\n",
        "This Colab notebook covers preprocessing, POS tagging, NER, tokenization, Seq2Seq model, and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aAtpXXIMxZNQ",
      "metadata": {
        "id": "aAtpXXIMxZNQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
